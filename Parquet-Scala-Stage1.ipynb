{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "\nimport com.ibm.ibmos2spark.bluemix\n\n// @hidden_cell\nvar credentials = scala.collection.mutable.HashMap[String, String](\n    \"auth_url\"->\"https://identity.open.softlayer.com\",\n    \"project_id\"->\"c23aa3b678b644dc9264ec14f49137e9\",\n    \"region\"->\"dallas\",\n    \"user_id\"->\"f03472a8715641af858cee0e04e0d253\",\n    \"password\"->\"ylX)R&9=xe*lGOf8\"\n)\n\nvar configurationname = \"os_a5a84a57bdfd407ca0092694bf86f990_configs\"\nvar bmos = new bluemix(sc, configurationname, credentials)\n\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\n// Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n// Please read the documentation of 'SparkSession.read()' and 'DataFrameReader' to learn more about the possibilities to adjust the data loading.\n// Spark documentation: http://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.sql.DataFrameReader@json%28paths:String*%29:org.apache.spark.sql.DataFrame\n\nval dfData1 = spark.read.parquet(bmos.url(\"MyProj1\", \"parquet_0.json\"))\ndfData1.show(5)\n", 
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+--------------------+-----------------+\n|stars|               texts|__index_level_0__|\n+-----+--------------------+-----------------+\n|    5|dr. goldberg offe...|                0|\n|    2|Unfortunately, th...|                1|\n|    4|Dr. Goldberg has ...|                2|\n|    4|Been going to Dr....|                3|\n|    4|Got a letter in t...|                4|\n+-----+--------------------+-----------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "\nimport com.ibm.ibmos2spark.bluemix\n\n// @hidden_cell\nvar credentials = scala.collection.mutable.HashMap[String, String](\n    \"auth_url\"->\"https://identity.open.softlayer.com\",\n    \"project_id\"->\"c23aa3b678b644dc9264ec14f49137e9\",\n    \"region\"->\"dallas\",\n    \"user_id\"->\"ad8c18ac39aa4a2fab35e96065370252\",\n    \"password\"->\"e-nVeqt_1pCs5zKh\"\n)\n\nvar configurationname = \"os_ad8c18ac39aa4a2fab35e96065370252_configs\"\nvar bmos = new bluemix(sc, configurationname, credentials)\n\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\n// Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n// Please read the documentation of 'SparkSession.read()' and 'DataFrameReader' to learn more about the possibilities to adjust the data loading.\n// Spark documentation: http://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.sql.DataFrameReader@json%28paths:String*%29:org.apache.spark.sql.DataFrame\n\nval dfData1 = spark.read.parquet(bmos.url(\"Output\", \"parquet_0.json\"))\ndfData1.show(5)", 
            "cell_type": "code", 
            "execution_count": 6, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+--------------------+-----------------+\n|stars|               texts|__index_level_0__|\n+-----+--------------------+-----------------+\n|    5|dr. goldberg offe...|                0|\n|    2|Unfortunately, th...|                1|\n|    4|Dr. Goldberg has ...|                2|\n|    4|Been going to Dr....|                3|\n|    4|Got a letter in t...|                4|\n+-----+--------------------+-----------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "metadata": {}
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.0", 
            "name": "scala-spark20", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }
}