{"metadata":{"guid":"2e438ecc-3946-438e-8f9a-f2f854982074","url":"/v2/streaming_pipelines/2e438ecc-3946-438e-8f9a-f2f854982074","created_at":"2018-01-14T14:27:08Z","updated_at":"2018-01-16T10:49:55Z","revision":1516099795081},"entity":{"name":"LDA_Topic_Classification","description":"","project_guid":"f4bbc994-33be-4d1c-a5a9-48c02017e338","graph":{"doc_type":"pipeline","version":"1.0","json_schema":"http://www.ibm.com/ibm/wdp/flow-v1.0/pipeline-flow-v1-schema.json","id":"","app_data":{"ui_data":{"name":"LDA_Topic_Classification"}},"primary_pipeline":"primary-pipeline","pipelines":[{"id":"primary-pipeline","runtime":"streams","nodes":[{"id":"messagehub_kqlwr3q484n","type":"binding","op":"ibm.streams.sources.messagehub","outputs":[{"id":"target","schema_ref":"schema0","links":[{"node_id_ref":"code_ml_5j86dd49anq","port_id_ref":"source"}]}],"parameters":{"schema_mapping":[{"name":"text","type":"string","length":255,"path":"/text"}]},"connection":{"ref":"c54f11c0-f84e-48cc-bacd-937e2feec18c","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"id":"newsData","name":"newsData","type":"topic","path":"/newsData"}}},"app_data":{"ui_data":{"label":"Message Hub","x_pos":-260,"y_pos":20}}},{"id":"code_ml_5j86dd49anq","type":"execution_node","op":"ibm.streams.operations.code-ml","outputs":[{"id":"target","schema_ref":"schema1","links":[{"node_id_ref":"objectstorage_v2_t5u28o9l8ur","port_id_ref":"source"}]}],"parameters":{"file_objects":[{"file_reference_name":"model","auto_refresh":false,"connection":{"ref":"b1858d71-ce16-4dfb-911b-cd924233872b","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"path":"/pyml/LDA_news.model.pkg.gz","asset_types":[],"assets":[],"fields":[],"extended_metadata":[],"first":{"href":"https://api.dataplatform.ibm.com/v2/connections/b1858d71-ce16-4dfb-911b-cd924233872b/assets?project_id=f4bbc994-33be-4d1c-a5a9-48c02017e338&offset=0&limit=100&path=%2Fpyml%2FLDA_news.model.pkg.gz"},"total_count":1,"logs":[{"severity":"error","message":"CDICO2063E: The content of the file is not in a supported format.","details":{}}]}}}}],"code":"#\n# YOU MUST EDIT THE SCHEMA and add all attributes that you are returning as output.\n#\n# Python libraries that are supported and selected (âœ“) at the \"In Installer\" list at\n# https://docs.continuum.io/anaconda/packages/pkg-docs\n\nimport sys\nfrom gensim import models, corpora, utils\n\n\n# init() function will be called once on pipeline initialization\n# @state a Python dictionary object for keeping state. The state object is passed to the process function\ndef init(state):\n\n    state['model'] = None\n    state['model_id'] = None\n    state['phraser'] = None\n    state['stoplist'] = setStopWordList()\n    state['lemmatizer'] = setLemmatizer()\n\n\n# process() function will be invoked on every event tuple\n# @event a Python dictionary object representing the input event tuple as defined by the input schema\n# @state a Python dictionary object for keeping state over subsequent function calls\n# return must be a Python dictionary object. It will be the output of this operator.\n#        Returning None results in not submitting an output tuple for this invocation.\n# You must declare all output attributes in the Edit Schema window.\ndef process(event, state):\n\n    output = None\n\n    if state['model'] and event['text'] and len(event['text']) > 0:\n        # Pre-process and cleanse the texts\n        textTokens = preprocess_text( event['text'], state['stoplist'], state['phraser'], state['lemmatizer'])\n        # Get the {topic, terms} for the current text\n        output = get_topic( state['model'], textTokens)\n\n    # Append the original text and the model_id\n    if output != None:\n        output['text' ] = event['text']\n        output['model_id'] = state['model_id']\n\n    return output\n\n\ndef load_model(state, path_model):\n    \"\"\"\n    Reverse the packaging done by the package_model() function in the notebook.\n    The gzip-ed package should contain the model, bigram_phraser and the package creation timestamp.\n    \"\"\"\n    import pickle, gzip\n\n    pkg = {}\n    try:\n        with open( path_model, 'rb') as pkg_file:\n            pkg_gz = pkg_file.read()\n            pkg = pickle.loads(gzip.decompress(pkg_gz))\n    except Exception as err:\n        print(err)\n\n    state['model_id'] = pkg.get('timestamp')\n    state['model']    = pkg.get('model')\n    state['phraser']  = pkg.get('phraser')\n\n\ndef preprocess_text(text, stoplist, bigram_phraser, lemmatizer=None):\n    \"\"\"\n    Steps to pre-process and cleanse a single text:\n    1. Stopword Removal.\n    2. Collocation detection (bigram).\n    3. Lemmatization (not stem since stemming can reduce the interpretability).\n    Parameters:\n    * text: a single text string.\n    * stoplist: list of stopword tokens (from nltk.corpus.stopwords.words('english'))\n    * bigram_phraser: the bigram_phraser which was packaged with the trained model\n    * lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer())\n    Returns:\n    * tokens: Pre-processed tokenized texts.\n    \"\"\"\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    tokens = [word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                     if word not in stoplist]\n\n    # bigram collocation detection\n    if bigram_phraser:\n        tokens = bigram_phraser[tokens]\n\n    if lemmatizer:\n        tokens = [word for word in lemmatizer.lemmatize(' '.join(tokens), pos='v').split()]\n\n    return tokens\n\n\ndef get_topic(model, textTokens):\n    \"\"\"\n    Use the model to determine the top topic for the preprocessed tokens of a text.\n    Return tuple with the topic, it's terms (describing the topic) and the original text.\n    \"\"\"\n    topicTerms = {}\n\n    # The Bag-of-Words method takes the input text tokens (words) and returns a list of tuples\n    # containing the word's token-id (within the model dictionary (id2word)) and the word's frequency within the input text.\n    textBOW = model.id2word.doc2bow(textTokens)\n\n    # Given the textBOW, use the model to get the top topic\n    topTopicId = max( model[textBOW], key=lambda topic:topic[1])[0]\n\n    # Retrieve the topic terms (top 20 most probable words) from the model to include with the output returned\n    topTopicTerms = model.print_topic(topTopicId, topn=20)\n\n    topicTerms['topic'] = topTopicId\n    topicTerms['terms'] = topTopicTerms\n\n    return topicTerms\n\n\ndef setStopWordList():\n    stoplist = {}\n    try:\n        import nltk\n        nltk.download(\"stopwords\")\n        stoplist = set(nltk.corpus.stopwords.words(\"english\"))\n    except:\n        stoplist = {}\n\n    if stoplist == {}: # Use a default, just in case\n        stoplist = {'because', 'during', 'was', 'itself', 'should', 'by', 'haven', 'yourself', 'been', 're', 'ain', 'hadn', 'had', 'again', 'what', 'they', 'themselves', 'whom', 'you', 'all', 'both', 'on', 'isn', 'his', 'ourselves', 'that', 't', 'm', 'is', 'this', 'how', 'when', 'will', 'against', 'her', 'with', 'couldn', 'being', 'hasn', 'be', 'it', 'but', 'no', 'than', 'don', 'most', 'now', 'while', 'doesn', 'our', 'from', 'are', 'he', 'so', 'shouldn', 've', 'y', 'as', 'we', 'll', 's', 'himself', 'my', 'about', 'more', 'where', 'down', 'there', 'just', 'nor', 'theirs', 'such', 'who', 'to', 'before', 'him', 'me', 'has', 'o', 'its', 'were', 'did', 'can', 'same', 'then', 'have', 'few', 'aren', 'd', 'other', 'further', 'and', 'off', 'these', 'an', 'wasn', 'hers', 'your', 'weren', 'until', 'only', 'does', 'shan', 'i', 'own', 'not', 'or', 'myself', 'through', 'some', 'didn', 'at', 'out', 'why', 'needn', 'doing', 'above', 'after', 'wouldn', 'yourselves', 'very', 'having', 'herself', 'a', 'the', 'am', 'if', 'into', 'once', 'won', 'too', 'up', 'ours', 'here', 'those', 'each', 'in', 'over', 'ma', 'them', 'under', 'for', 'mustn', 'yours', 'mightn', 'below', 'between', 'which', 'do', 'any', 'she', 'of', 'their'}\n\n    return stoplist\n\n\ndef setLemmatizer():\n    lemmatizer = None\n    try:\n        import nltk\n        nltk.download(\"wordnet\")\n        lemmatizer = nltk.stem.WordNetLemmatizer()\n    except:\n        pass\n\n    return lemmatizer","schema_mapping":[{"name":"model_id","type":"string","length":0,"source_elem_name":"","target_elem_name":""},{"name":"topic","type":"double","length":0,"source_elem_name":"","target_elem_name":""},{"name":"terms","type":"string","length":0,"source_elem_name":"","target_elem_name":""},{"name":"text","label":"counter (Number)","type":"string"}]},"app_data":{"ui_data":{"label":"Python Machine learning","x_pos":20,"y_pos":100}}},{"id":"objectstorage_v2_t5u28o9l8ur","type":"binding","op":"ibm.streams.targets.objectstorage_v2","parameters":{"format":"csv","write_policy":"numberOfEvents","rolling_number_of_events":"1000"},"connection":{"ref":"b1858d71-ce16-4dfb-911b-cd924233872b","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"id":"/pyml-output/LDA_news.topics.%TIME.csv","name":"/pyml-output/LDA_news.topics.%TIME.csv","path":"/pyml-output/LDA_news.topics.%TIME.csv","type":"unknown"}}},"app_data":{"ui_data":{"label":"Cloud Object Storage","x_pos":270,"y_pos":150}}}]}],"schemas":[{"id":"schema0","fields":[{"name":"text","type":"string"}]},{"id":"schema1","fields":[{"name":"model_id","type":"string"},{"name":"topic","type":"double"},{"name":"terms","type":"string"},{"name":"text","type":"string"}]}]},"engines":{"streams":{"instance_id":"779ee6ed-4f31-40d2-b59a-7a277c32a2d6"}}}}