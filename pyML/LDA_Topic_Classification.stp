{"metadata":{"guid":"2e438ecc-3946-438e-8f9a-f2f854982074","url":"/v2/streaming_pipelines/2e438ecc-3946-438e-8f9a-f2f854982074","created_at":"2018-01-14T14:27:08Z","updated_at":"2018-01-29T10:44:00Z","revision":1517222640165},"entity":{"name":"LDA_Topic_Classification","description":"","project_guid":"f4bbc994-33be-4d1c-a5a9-48c02017e338","graph":{"doc_type":"pipeline","version":"1.0","json_schema":"http://www.ibm.com/ibm/wdp/flow-v1.0/pipeline-flow-v1-schema.json","id":"","app_data":{"ui_data":{"name":"LDA_Topic_Classification"}},"primary_pipeline":"primary-pipeline","pipelines":[{"id":"primary-pipeline","runtime":"streams","nodes":[{"id":"messagehub_kqlwr3q484n","type":"binding","op":"ibm.streams.sources.messagehub","outputs":[{"id":"target","schema_ref":"schema0","links":[{"node_id_ref":"code_ml_5j86dd49anq","port_id_ref":"source"}]}],"parameters":{"schema_mapping":[{"name":"text","type":"string","length":255,"path":"/text"}]},"connection":{"ref":"c54f11c0-f84e-48cc-bacd-937e2feec18c","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"id":"newsData","name":"newsData","type":"topic","path":"/newsData"}}},"app_data":{"ui_data":{"label":"Message Hub","x_pos":-260,"y_pos":20}}},{"id":"code_ml_5j86dd49anq","type":"execution_node","op":"ibm.streams.operations.code-ml","outputs":[{"id":"target","schema_ref":"schema1","links":[{"node_id_ref":"objectstorage_v2_t5u28o9l8ur","port_id_ref":"source"}]}],"parameters":{"file_objects":[{"file_reference_name":"model","auto_refresh":false,"connection":{"ref":"b1858d71-ce16-4dfb-911b-cd924233872b","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"path":"/pyml/LDA_news.model.pkg.gz","asset_types":[],"assets":[],"fields":[],"extended_metadata":[],"first":{"href":"https://api.dataplatform.ibm.com/v2/connections/b1858d71-ce16-4dfb-911b-cd924233872b/assets?project_id=f4bbc994-33be-4d1c-a5a9-48c02017e338&offset=0&limit=100&path=%2Fpyml%2FLDA_news.model.pkg.gz"},"total_count":1,"logs":[{"severity":"error","message":"CDICO2063E: The content of the file is not in a supported format.","details":{}},{"severity":"error","message":"CDICO2063E: The content of the file is not in a supported format.","details":{}}]}}}}],"code":"#\n# YOU MUST EDIT THE SCHEMA and add all attributes that you are returning as output.\n#\n# Python libraries that are supported and selected (âœ“) at the \"In Installer\" list at\n# https://docs.continuum.io/anaconda/packages/pkg-docs\n\nimport sys\nfrom gensim import models, corpora, utils\n\n\n# init() function will be called once on pipeline initialization\n# @state a Python dictionary object for keeping state. The state object is passed to the process function\ndef init(state):\n\n    state['model'] = None\n    state['model_id'] = None\n    state['phraser'] = None\n    state['stoplist'] = setStopWordList()\n    state['lemmatizer'] = setLemmatizer()\n\n\n# process() function will be invoked on every event tuple\n# @event a Python dictionary object representing the input event tuple as defined by the input schema\n# @state a Python dictionary object for keeping state over subsequent function calls\n# return must be a Python dictionary object. It will be the output of this operator.\n#        Returning None results in not submitting an output tuple for this invocation.\n# You must declare all output attributes in the Edit Schema window.\ndef process(event, state):\n\n    output = None\n\n    if state['model'] and event['text'] and len(event['text']) > 0:\n        # Pre-process and cleanse the texts\n        textTokens = preprocess_text( event['text'], state['stoplist'], state['phraser'], state['lemmatizer'])\n        # Get the {topic, terms} for the current text\n        output = get_topic( state['model'], textTokens)\n\n    # Append the original text and the model_id\n    if output != None:\n        output['text' ] = event['text']\n        output['model_id'] = state['model_id']\n\n    return output\n\n\ndef load_model(state, path_model):\n    \"\"\"\n    Reverse the packaging done by the package_model() function in the notebook.\n    The gzip-ed package should contain the model, bigram_phraser and the package creation timestamp.\n    \"\"\"\n    import pickle, gzip\n\n    pkg = {}\n    try:\n        with open( path_model, 'rb') as pkg_file:\n            pkg_gz = pkg_file.read()\n            pkg = pickle.loads(gzip.decompress(pkg_gz))\n    except Exception as err:\n        print(err)\n\n    state['model_id'] = pkg.get('timestamp')\n    state['model']    = pkg.get('model')\n    state['phraser']  = pkg.get('phraser')\n\n\ndef preprocess_text(text, stoplist, bigram_phraser, lemmatizer=None):\n    \"\"\"\n    Steps to pre-process and cleanse a single text:\n    1. Stopword Removal.\n    2. Collocation detection (bigram).\n    3. Lemmatization (not stem since stemming can reduce the interpretability).\n    Parameters:\n    * text: a single text string.\n    * stoplist: list of stopword tokens (from nltk.corpus.stopwords.words('english'))\n    * bigram_phraser: the bigram_phraser which was packaged with the trained model\n    * lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer())\n    Returns:\n    * tokens: Pre-processed tokenized texts.\n    \"\"\"\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words and header fields.\n    tokens = [word for word in utils.tokenize(re.sub(r'(?i)(From|Organization|Lines|Nntp-Posting-Host|X-Newsreader): [^\\n]*(\\n)', '', text),\n                                              lowercase=True, deacc=True, errors=\"ignore\")\n                    if word not in stoplist]\n\n    # bigram collocation detection\n    if bigram_phraser:\n        tokens = bigram_phraser[tokens]\n\n    if lemmatizer:\n        tokens = [word for word in lemmatizer.lemmatize(' '.join(tokens), pos='v').split()]\n\n    return tokens\n\n\ndef get_topic(model, textTokens):\n    \"\"\"\n    Use the model to determine the top topic for the preprocessed tokens of a text.\n    Return tuple with the topic, it's terms (describing the topic) and the original text.\n    \"\"\"\n    topicTerms = {}\n\n    # The Bag-of-Words method takes the input text tokens (words) and returns a list of tuples\n    # containing the word's token-id (within the model dictionary (id2word)) and the word's frequency within the input text.\n    textBOW = model.id2word.doc2bow(textTokens)\n\n    # Given the textBOW, use the model to get the top topic\n    topTopicId = max( model[textBOW], key=lambda topic:topic[1])[0]\n\n    # Retrieve the topic terms (top 20 most probable words) from the model to include with the output returned\n    topTopicTerms = model.print_topic(topTopicId, topn=20)\n\n    topicTerms['topic'] = topTopicId\n    topicTerms['terms'] = topTopicTerms\n\n    return topicTerms\n\n\ndef setStopWordList():\n    stoplist = {}\n    try:\n        import nltk\n        from sklearn.feature_extraction import stop_words\n        from stop_words import get_stop_words\n\n        # sklearn has 318 stop words\n        stoplist = set(stop_words.ENGLISH_STOP_WORDS)\n\n        # nltk has 179 stop words\n        nltk.download(\"stopwords\")\n        stoplist.update(nltk.corpus.stopwords.words(\"english\"))\n\n        # stop_words has 174 stop words\n        stoplist.update(get_stop_words('english'))\n    except:\n        stoplist = {}\n\n    if stoplist == {}: # Use a default, just in case\n        stoplist = {'eight', 'still', 'name', 'was', 'how', 'for', 'could', \"when's\", 'please', 'inc', 'then', 'nobody', 'whereupon', 'so', 'mine', 'top', 'as', 'any', 'somehow', 'you', 'eg', 'why', 'several', 'been', 'fire', 'six', 'sometimes', 'by', 'where', 'get', 'hence', 'yourself', 'hereafter', 'wasn', 'three', 'did', \"there's\", 'towards', 'whose', 'made', 'may', 'side', 'thereafter', 'hasn', 'doing', 'beside', 'show', 'noone', 'none', 'be', 'anywhere', 'further', 'interest', 'toward', 'due', 'her', \"you'll\", 'yourselves', 'my', 'what', 'nothing', 'not', 'thereby', 'beforehand', 'there', 'forty', 'each', \"we're\", 'system', 'thence', 'whether', 'haven', \"we've\", 'serious', 'perhaps', 'me', \"how's\", 've', 'together', 'weren', 'four', 'nevertheless', 'rather', 'elsewhere', \"should've\", 'con', 'among', 'at', 'former', 'seemed', 'such', 't', 'sincere', 'up', \"she'd\", 'than', 'except', 'via', 'can', 'everything', \"you've\", 'already', 'along', 'something', 'while', 'ourselves', 'anyone', 'our', 'thus', 'amoungst', 'keep', \"wouldn't\", 'must', 'either', 'y', 'shouldn', 'another', 'fill', 'move', 'onto', 'doesn', 'detail', 'latterly', 'ain', 'between', \"mightn't\", 'had', 'ever', 'does', 'or', \"mustn't\", 'only', 'whereafter', 'twelve', 'won', 'needn', 'hundred', 'whenever', 'also', 'in', 'see', \"needn't\", 'whither', \"aren't\", 'less', 'again', 'much', 'others', \"she'll\", 'll', 'should', 'itself', 'themselves', 'although', 'around', 'sometime', 'until', 'whence', 'seem', 'mill', 'no', 'she', 'us', \"they'll\", \"they're\", 'amongst', \"i'll\", 'of', 'down', 'five', 'were', 'whole', 'couldnt', 'them', 'nine', 'everywhere', 'under', 'shan', 'couldn', 'on', 'an', \"you'd\", 'therein', 'yours', 'go', 'would', 'very', 'empty', \"weren't\", 'beyond', 'through', 'become', 'most', 'wherever', 'etc', 'cannot', \"you're\", \"it's\", \"doesn't\", 'here', 's', 'namely', \"don't\", \"shan't\", 'per', 'sixty', 'mostly', 'during', \"she's\", 'whoever', 'out', 'we', \"let's\", 'myself', 'fify', 'they', 'about', \"hasn't\", 'ie', 'find', 'both', \"i'd\", 'since', 'put', 're', 'ten', 'don', 'm', 'mightn', 'amount', 'anyhow', 'after', 'back', 'whatever', 'whereby', 'wouldn', 'though', 'thru', \"isn't\", \"that's\", 'a', 'everyone', \"haven't\", 'done', 'often', \"hadn't\", 'call', 'never', 'cant', 'hasnt', 'thin', \"he'll\", 'their', \"they'd\", 'even', 'bill', 'latter', 'might', 'seems', 'ours', 'now', 'twenty', 'am', 'take', \"can't\", 'someone', 'fifteen', 'that', 'last', 'to', 'd', \"won't\", \"he's\", \"couldn't\", 'every', 'own', 'becoming', 'upon', 'one', 'becomes', 'this', 'all', 'meanwhile', 'more', 'else', \"who's\", 'which', 'full', 'himself', 'indeed', 'nor', 'de', 'give', 'moreover', 'bottom', 'other', 'almost', 'well', 'hereupon', \"shouldn't\", 'became', 'who', 'nowhere', 'same', \"here's\", \"i'm\", 'alone', 'too', 'these', \"wasn't\", 'before', \"didn't\", 'he', 'i', 'its', 'but', 'with', 'because', 'those', 'the', 'it', 'thick', 'first', 'mustn', 'hers', 'herein', 'thereupon', 'third', 'just', 'always', 'over', 'aren', \"we'll\", 'have', 'and', \"we'd\", 'some', 'when', 'eleven', \"i've\", 'below', 'if', 'theirs', 'behind', 'isn', 'hereby', 'his', \"what's\", 'whom', 'hadn', 'above', 'are', 'will', \"where's\", 'from', 'into', 'enough', 'part', 'least', 'your', 'anyway', 'describe', 'found', 'few', 'un', 'formerly', 'herself', 'however', 'has', 'ought', 'within', 'cry', 'once', 'seeming', 'ma', 'having', 'co', 'somewhere', \"why's\", 'him', 'neither', 'front', 'do', 'whereas', 'anything', 'afterwards', 'being', 'off', 'o', \"that'll\", 'across', 'besides', 'wherein', \"they've\", 'didn', 'next', 'throughout', 'against', 'two', 'otherwise', 'is', 'yet', 'without', 'therefore', \"he'd\", 'many', 'ltd'}\n\n    return stoplist\n\n\ndef setLemmatizer():\n    lemmatizer = None\n    try:\n        import nltk\n        nltk.download(\"wordnet\")\n        lemmatizer = nltk.stem.WordNetLemmatizer()\n    except:\n        pass\n\n    return lemmatizer","schema_mapping":[{"name":"model_id","type":"string","length":0,"source_elem_name":"","target_elem_name":""},{"name":"topic","type":"double","length":0,"source_elem_name":"","target_elem_name":""},{"name":"terms","type":"string","length":0,"source_elem_name":"","target_elem_name":""},{"name":"text","label":"counter (Number)","type":"string"}]},"app_data":{"ui_data":{"label":"Python Machine learning","x_pos":20,"y_pos":100}}},{"id":"objectstorage_v2_t5u28o9l8ur","type":"binding","op":"ibm.streams.targets.objectstorage_v2","parameters":{"format":"csv","write_policy":"numberOfEvents","rolling_number_of_events":"1000"},"connection":{"ref":"b1858d71-ce16-4dfb-911b-cd924233872b","project_ref":"f4bbc994-33be-4d1c-a5a9-48c02017e338","properties":{"asset":{"id":"/pyml-output/LDA_news.topics.%TIME.csv","name":"/pyml-output/LDA_news.topics.%TIME.csv","path":"/pyml-output/LDA_news.topics.%TIME.csv","type":"unknown"}}},"app_data":{"ui_data":{"label":"Cloud Object Storage","x_pos":270,"y_pos":150}}}]}],"schemas":[{"id":"schema0","fields":[{"name":"text","type":"string"}]},{"id":"schema1","fields":[{"name":"model_id","type":"string"},{"name":"topic","type":"double"},{"name":"terms","type":"string"},{"name":"text","type":"string"}]}]},"engines":{"streams":{"instance_id":"779ee6ed-4f31-40d2-b59a-7a277c32a2d6"}}}}