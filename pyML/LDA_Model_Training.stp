{"metadata":{"guid":"ceeba78c-98db-4fa6-99de-3b8eaa4ce019","url":"/v2/streaming_pipelines/ceeba78c-98db-4fa6-99de-3b8eaa4ce019","created_at":"2017-12-04T14:38:28Z","updated_at":"2017-12-04T14:41:13Z","revision":1512398473464},"entity":{"name":"LDA_Model_Training","description":"","project_guid":"50ac5cb5-7ca8-46c5-9429-4b348df1f962","graph":{"doc_type":"pipeline","version":"1.0","json_schema":"http://www.ibm.com/ibm/wdp/flow-v1.0/pipeline-flow-v1-schema.json","id":"","app_data":{"ui_data":{"name":"LDA_Model_Training"}},"primary_pipeline":"primary-pipeline","pipelines":[{"id":"primary-pipeline","runtime":"streams","nodes":[{"id":"messagehub_kqlwr3q484n","type":"binding","op":"ibm.streams.sources.messagehub","outputs":[{"id":"target","schema_ref":"schema0","links":[{"node_id_ref":"code_2lmn432kctt","port_id_ref":"source"}]}],"parameters":{"schema_mapping":[{"name":"text","type":"string","length":255,"path":"/text"},{"name":"stars","type":"double","path":"/stars"}]},"connection":{"ref":"5cc71be6-6cd3-47e3-ae67-85fab0516ec5","project_ref":"50ac5cb5-7ca8-46c5-9429-4b348df1f962","properties":{"asset":{"id":"testTopic1","name":"testTopic1","type":"topic","path":"/testTopic1"}}},"app_data":{"ui_data":{"label":"Message Hub","x_pos":-260,"y_pos":20}}},{"id":"code_2lmn432kctt","type":"execution_node","op":"ibm.streams.operations.code","outputs":[{"id":"target","schema_ref":"schema1","links":[{"node_id_ref":"objectstorage_v2_5ijajgslfya","port_id_ref":"source"}]}],"parameters":{"code":"#\n# YOU MUST EDIT THE SCHEMA and add all attributes that you are returning as output.\n#\n# Python libraries that are supported and selected (âœ“) at the \"In Installer\" list at\n# https://docs.continuum.io/anaconda/packages/pkg-docs\n\nimport sys\nimport requests\nimport os\nfrom gensim import models, corpora, utils\n\n####################### TODO: remove this section when codegen will fix it ############################################\n\n#resp = requests.get('https://raw.githubusercontent.com/Yura32000/practicals/master/watson_streaming_pipelines.py')\nresp = requests.get('https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py')\nwith open('watson_streaming_pipelines.py', 'w') as f:\n    f.write(resp.text)\n\ndir = os.path.dirname(os.path.realpath('__file__'))\nsys.path.insert(0, dir)\n\nimport watson_streaming_pipelines as wstp\n\n########################################################################################################################\n\n# Provide credentials for Cloud Object Storage\ncos_credentials = {\n    #'api_key':'<FILL>',\n    #'resource_instance_id':'<FILL crn:..::>',\n    'api_key':'xhjheSC7AhSLtvapSDnbyFn17uWUqW5ccAOuHhQxnnEY',\n    'resource_instance_id':'crn:v1:staging:public:cloud-object-storage:global:a/68a66698d275aeb48097f868957ab2ed:bbb5aa36-5525-4000-b129-bcb780195098::',\n    'iam_url':'https://iam.stage1.ng.bluemix.net/oidc/token',\n    'url':'https://s3-api.us-geo.objectstorage.uat.service.networklayer.com',\n    'endpoint':'https://s3.us-west.objectstorage.uat.softlayer.net'\n}\n\n# If a model file was loaded, the topic classification of each tuple's text will be returned.\n\n# Number of streaming tuples (texts) to skip before collecting them for re-training.\ntrain_skip = 10000\n# Number of texts to collect for re-training.\ntrain_tuples = 10000\n\n# Bucket and file name for the model. If it doesn't exist, it will be created and constantly updated.\nmodel_bucket_name = 'pyml'\nmodel_file_name = 'LDA_test_2updated.model.pickle'\n\n\n# init() function will be called once on pipeline initialization\n# @state a Python dictionary object for keeping state. The state object is passed to the process function\ndef init(state):\n\n    state['stoplist'] = wstp.setStopWordList()\n\n    state['model'] = wstp.get_from_cos( cos_credentials, model_bucket_name + \"/\" + model_file_name,\n                                        serializer=lambda v: wstp.deserializePickle(v))\n\n    # If the model did not exist, skip the skip-count so that training will begin immediately.\n    state['skip-count'] = 0 if state['model'] else train_skip\n    state['train-count'] = 0\n    state['texts'] = []\n\n\n# process() function will be invoked on every event tuple\n# @event a Python dictionary object representing the input event tuple as defined by the input schema\n# @state a Python dictionary object for keeping state over subsequent function calls\n# return must be a Python dictionary object. It will be the output of this operator.\n#        Returning None results in not submitting an output tuple for this invocation.\n# You must declare all output attributes in the Edit Schema window.\ndef process(event, state):\n\n    output = None\n\n    state['skip-count'] += 1\n    if state['skip-count'] > train_skip:\n        state['train-count'] += 1\n        state['texts'].append( event['text'])\n\n    if state['train-count'] == train_tuples:\n\n        model, output = train_model( state['model'], state['stoplist'], state['texts'])\n\n        if model:\n            state['model'] = model\n            wstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + model_file_name,\n                             wstp.pickleSerializer( data=model, zip=True))\n                             #gzip.compress( pickle.dumps( model, pickle.HIGHEST_PROTOCOL)))\n\n        state['skip-count'] = 0\n        state['train-count'] = 0\n        state['texts'] = []\n\n    return output\n\n# Train the model (from scratch or update).\n# Texts is a list of accumulated texts (size of list determined by global train_tuples)\n# Returns the model and a list of top topics and their terms.\ndef train_model( model, stoplist, texts):\n\n    # Preprocessing and cleansing\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    textTokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                         if word not in stoplist]\n                  for text in texts]\n\n    trainSize = len(textTokens)\n    # Optional: Use the above line to train all of the collected corpus, OR the following line to limit the training to 20% (but not less than 1000)\n    # trainSize = max( int(len(textTokens) * 0.20), 1000)\n\n    if not model:\n        # Create the dictionary\n        dict = corpora.HashDictionary( documents=textTokens)\n        # Optional: Filter out tokens which are in less than 5 and more than 90.0% of the documents\n        # dict.filter_extremes(no_below=5, no_above=0.9, keep_n=100000)\n\n        # The training corpus is the result of the Bag-of-Words method.\n        # The BOW method takes the text tokens (words) and returns a list of tuples containing\n        # the word's token-id within the dictionary, and it's frequency within the input text.\n        textBOW = [dict.doc2bow(text) for text in textTokens[:trainSize]]\n        # Create the gensim LDA model - choose best arguments\n        model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dict,\n                                          num_topics=20, update_every=0.5,\n                                          iterations=100, passes=3)\n#                                          iterations=10, passes=1) # ONLY FOR FASTER TESTING\n    else:\n        # Add the documents to the existing dictionary, get BOW corpus then update model.\n        model.id2word.add_documents(textTokens)\n        newCorpus = [model.id2word.doc2bow(text) for text in textTokens[:trainSize]]\n        model.update(newCorpus)\n\n    # Retrieve the topic terms from the model to include with the returned output\n    topicTerms = model.print_topics(num_topics=-1, num_words=20)\n\n    return model, topicTerms\n","schema_mapping":[{"name":"topic","type":"double","length":0,"source_elem_name":""},{"name":"terms","type":"string","length":0,"source_elem_name":"","target_elem_name":""}]},"app_data":{"ui_data":{"label":"Code","x_pos":20,"y_pos":-60}}},{"id":"objectstorage_v2_5ijajgslfya","type":"binding","op":"ibm.streams.targets.objectstorage_v2","parameters":{"header_row_enabled":true,"write_policy":"numberOfEvents","rolling_number_of_events":"20"},"connection":{"ref":"212176ed-cb0d-400e-a7d7-007e4fd8364a","project_ref":"50ac5cb5-7ca8-46c5-9429-4b348df1f962","properties":{"asset":{"path":"/output/pyML04-topicTerms-%TIME.csv","asset_types":[],"assets":[],"fields":[],"extended_metadata":[],"first":{"href":"https://apsx-api-dev.stage1.ng.bluemix.net/v2/connections/212176ed-cb0d-400e-a7d7-007e4fd8364a/assets?project_id=50ac5cb5-7ca8-46c5-9429-4b348df1f962&offset=0&limit=100&path=%2Foutput%2FpyML04-topicTerms-%25TIME.csv"},"total_count":0,"logs":[]}}},"app_data":{"ui_data":{"label":"Cloud Object Storage","x_pos":270,"y_pos":-130}}}]}],"schemas":[{"id":"schema0","fields":[{"name":"text","type":"string"},{"name":"stars","type":"double"}]},{"id":"schema1","fields":[{"name":"topic","type":"double"},{"name":"terms","type":"string"}]}]},"engines":{"streams":{"instance_id":"8f40c594-150e-4374-9ff4-71bd6d28f642"}}}}