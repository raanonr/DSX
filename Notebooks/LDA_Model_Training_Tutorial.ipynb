{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Machine Learning Model for Unsupervised Text Classification\n",
    "Part 1 of 3\n",
    "\n",
    "![pyLDAvis_2.png](https://github.com/raanonr/DSX/blob/master/pyML/pyLDAvis_2.png?raw=true \"pyLDAvis image2\")\n",
    "\n",
    "## Introduction\n",
    "In this tutorial notebook we will use the [gensim](https://radimrehurek.com/gensim/about.html) library to create a [Latent Dirichlet allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) machine learning [topic model](https://en.wikipedia.org/wiki/Topic_model). LDA is an _unsupervised_ model, so the topics which are identified during training are assigned a number (for a _label_) and described by a group of terms, where each term is coupled with a weight.\n",
    "\n",
    "We will use a sample newsgroup dataset [provided by gensim](https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/) for the purposes of training and scoring.\n",
    "***\n",
    "In part 1 of this tutorial notebook series, we will create/train the model, visualize it with [pyLDAvis](http://pyldavis.readthedocs.io/en/latest/) and then save the model on __Bluemix Cloud Object Storage__.\n",
    "\n",
    "In part 2[LINK], we will create a __Bluemix Streams flow for Streaming Analytics__, which uses the model to perform real-time topic prediction on news items from the dataset. The results will be also be saved to __Bluemix Cloud Object Storage__.\n",
    "\n",
    "In part 3[LINK], we will visualize the results of the topic predictions.....\n",
    "\n",
    "Some familiarity with Python is recommended. This notebook has been verified for Python 3.5, Spark 2.1 and gensim 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"TOC\"></a> Table of contents\n",
    "1. [Setup](#setup)<br>\n",
    "    1.1 [Provide Credentials for Cloud Object Storage (COS)](#setup1)<br>\n",
    "    1.2 [Provide COS bucket and object names](#setup2)<br>\n",
    "    1.3 [Download the sample dataset](#setup3)<br>\n",
    "2. [Define functions](#load)<br>\n",
    "    2.1 [function: read_dataset](#define1)<br>\n",
    "    2.2 [function: preprocess_texts](#define2)<br>\n",
    "    2.3 [function: train_model](#define3)<br>\n",
    "    2.4 [function: package_mode](#define4)<br>\n",
    "    2.5 [function: save_to_cos](#define5)<br>\n",
    "3. [Create the model](#train)<br>\n",
    "4. [Display a visualization of the topic model](#visualize)<br>\n",
    "5. [Save the model and topic terms to Cloud Object Storage](#save)<br>\n",
    "6. [Summary and next steps](#summary)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. [Setup](#TOC)\n",
    "\n",
    "In the tutorial setup, we will provide credentials and download the helper functions and sample dataset.\n",
    "\n",
    "<a id=\"setup1\"></a>\n",
    "### 1.1 [Provide Credentials for Cloud Object Storage (COS)](#TOC)\n",
    "The COS credentials will be used at the end of this tutorial for storing the created model.\n",
    "\n",
    "In order to insert your credentials into the notebook, open the `Connections` tab to the right.\n",
    "* Use the notebook menu bar to open the `Data` panel on the right.\n",
    "* Select the `Connections` tab.\n",
    "***\n",
    "* Click in the empty cell below and select <font color=blue>__insert to code__</font> for your __Cloud Object Storage__ service.\n",
    "* <font color=red>Change the *name*</font> of the COS credentials variable to __cos_credentials__.\n",
    "* The dictionary variable should contain the following keys: *iam_url, api_key, resource_instance_id, and url*.\n",
    "* Add an <font color=red>*additional key*</font> to the variable called __endpoint__.\n",
    "   * Get the the endpoint value from your Cload Object Storage (COS) service's page.\n",
    "      * From your Bluemix [Dashboard](https://console.bluemix.net/dashboard/apps), select the COS service.\n",
    "      * Select `Endpoint` from the menu on the left.\n",
    "      * Choose the __PUBLIC__ endpoint for your *location* (for example, *us-geo*).\n",
    "      * Prefix the endpoint value with \"https://\"\n",
    "\n",
    "Your variable should look like this...\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "```\n",
    "cos_credentials = {\n",
    "  'iam_url':'<YOUR-VALUE>',\n",
    "  'api_key':'<YOUR-VALUE>',\n",
    "  'resource_instance_id':'<YOUR-VALUE>',\n",
    "  'url':'<YOUR-VALUE>',\n",
    "\n",
    "  'endpoint':'https://<YOUR-VALUE>'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<INSERT YOUR COS CREDENTIALS, AS EXPLAINED ABOVE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup2\"></a>\n",
    "### 1.2 [Provide COS bucket and object names](#TOC)\n",
    "Choose a COS bucket name and object names for your packaged model (.gz) and topic-terms (.csv) files. \n",
    "<font color=red>Be sure that the bucket already exists!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_bucket_name = 'pyml'\n",
    "model_object_name = 'LDA_news.model.pkg.gz'\n",
    "topic_object_name = 'LDA_news.topic_terms.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RAANON\n",
    "cos_credentials_prod_ki = {\n",
    "  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n",
    "  'api_key':'0s-JWmaDBwiSd_yWJqenoKRBfTVU5Rgkz31CDT5WgoWQ',\n",
    "  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/db0d062d2b4c0836e18618a5222d8068:22e3b946-6154-4032-8e8f-7cfb0b429602::',\n",
    "  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "      \"endpoint\":\"https://s3-api.us-geo.objectstorage.softlayer.net\",\n",
    "}\n",
    "cos_credentials_stage1_wd = {\n",
    "  'iam_url':'https://iam.stage1.ng.bluemix.net/oidc/token',\n",
    "  'api_key':'xhjheSC7AhSLtvapSDnbyFn17uWUqW5ccAOuHhQxnnEY',\n",
    "  'resource_instance_id':'crn:v1:staging:public:cloud-object-storage:global:a/68a66698d275aeb48097f868957ab2ed:bbb5aa36-5525-4000-b129-bcb780195098::',\n",
    "  'url':'https://s3-api.us-geo.objectstorage.uat.service.networklayer.com',\n",
    "    'endpoint':'https://s3.us-west.objectstorage.uat.softlayer.net'\n",
    "}\n",
    "\n",
    "#cos_credentials = cos_credentials_prod_ki\n",
    "cos_credentials = cos_credentials_stage1_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify credential variables raise Exception('Missing value')\n",
    "#f = lambda v: [print(v) if len(v) == 0 for v in cos_credentials ]\n",
    "for k in cos_credentials:\n",
    "    print('Missing value for', k) if len(cos_credentials.get(k)) == 0 else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup3\"></a>\n",
    "### 1.3 [Download the sample dataset](#TOC)\n",
    "Version 3.2 of gensim (December 2017) includes a mechanism for [downloading](https://radimrehurek.com/gensim/downloader.html) some sample datasets.\n",
    "Even if you have a previous version of gensim, you can still download the sample dataset directly from the gensim (RaRe-Technologies) github repository (based on the source code at https://github.com/RaRe-Technologies/gensim/blob/master/gensim/downloader.py).\n",
    "\n",
    "We will use the __20-newsgroup__ dataset, which is described as: \"The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_BASE_URL = \"https://github.com/RaRe-Technologies/gensim-data/releases/download\"\n",
    "dataset=\"20-newsgroups\"\n",
    "\n",
    "#!rm -f {dataset}.gz*\n",
    "![[ ! -f {dataset}.gz ]] && wget '{DOWNLOAD_BASE_URL}/{dataset}/{dataset}.gz'\n",
    "!pwd && ls -l {dataset}.gz*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define\"></a>\n",
    "## 2. [Define functions](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define1\"></a>\n",
    "### 2.1 [function: read_dataset](#TOC)\n",
    "Load the dataset and create a List of texts. (All stored in memory, so assume a small dataset.) \n",
    "The dataset file should be in JSON format and contain a key called 'data'.\n",
    "\n",
    "Parameters:\n",
    "* dataset_path: Path and filename of the dataset file.\n",
    "* max_lines: If greater than 0, abort reading the file after max_lines lines.\n",
    "\n",
    "Returns:\n",
    "* data: List of the text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_dataset(dataset_path, max_lines=0):\n",
    "    \"\"\"\n",
    "    Read the dataset and return a List of each 'data' entry.\n",
    "    \"\"\"\n",
    "    from smart_open import smart_open\n",
    "    import json\n",
    "\n",
    "    print(\"opening...\", dataset_path)\n",
    "    \n",
    "    data = []\n",
    "    with smart_open( dataset_path, 'rb') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            if max_lines > 0 and i == max_lines:\n",
    "                break\n",
    "            jsonData = json.loads(line.decode('utf8'))\n",
    "            data.append(jsonData['data'])\n",
    "        infile.close()\n",
    "\n",
    "    print(len(data), \"lines read\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define2\"></a>\n",
    "### 2.2  [function: preprocess_texts](#TOC)\n",
    "Steps to pre-process and cleanse texts:\n",
    "1. Stopword Removal.\n",
    "2. Collocation detection (bigram).\n",
    "3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
    "    \n",
    "Parameters:\n",
    "* texts: List of texts.\n",
    "* stoplist: List of stopword tokens (from nltk.corpus.stopwords.words('english')).\n",
    "* lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer()).    \n",
    "\n",
    "Returns:\n",
    "* tokens: Pre-processed tokenized texts.\n",
    "* bigram_phraser: The bigram phraser which was created using all of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/gensim_news_classification.ipynb\n",
    "def preprocess_texts(texts, stoplist, lemmatizer=None):\n",
    "\n",
    "    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n",
    "    tokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n",
    "                     if word not in stoplist]\n",
    "               for text in texts]\n",
    "\n",
    "    # bigram collocation detection\n",
    "    bigram = models.Phrases(tokens)\n",
    "    bigram_phraser = models.phrases.Phraser(bigram)\n",
    "    tokens = [bigram_phraser[text] for text in tokens]\n",
    "\n",
    "    if lemmatizer:\n",
    "        tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n",
    "\n",
    "    return tokens, bigram_phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define3\"></a>\n",
    "### 2.3 [function: train_model](#TOC)\n",
    "Steps to create the model\n",
    "1. Create a Dictionary using the List of cleansed tokenized text.\n",
    "2. [optional] Filter extremes.\n",
    "3. Create a corpus from the Bag-of-Words method.  \n",
    "    The BOW method takes the text tokens (words) and returns a list of tuples containing  \n",
    "    the word's token-id within the dictionary, and it's frequency within the input text.\n",
    "4. Create and train an LDA model. Play around with the hyperparameters to affect speed and quality.\n",
    "\n",
    "Parameters:\n",
    "* textTokens: List of List of tokens, which are the cleansed text documents.\n",
    "\n",
    "Results:\n",
    "* model: The trained LDA model.\n",
    "* dictionary: The dictionary created from the tokenized text.\n",
    "* textBOW: The corpus used in creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model( textTokens):\n",
    "\n",
    "    # Create the dictionary\n",
    "    dictionary = corpora.Dictionary( documents=textTokens)\n",
    "    \n",
    "    # Optional: Filter out tokens which are in less than 10 and more than 75.0% of the documents\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.75, keep_n=50000)\n",
    "\n",
    "    # The training corpus is the result of the Bag-of-Words method.\n",
    "    textBOW = [dictionary.doc2bow(text) for text in textTokens]\n",
    "\n",
    "    # Create the gensim LDA model. Choose the best hyper-parameters. For example, try: iterations=100, passes=3\n",
    "    model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dictionary,\n",
    "                                      num_topics=20, update_every=0.5,\n",
    "                                      iterations=10, passes=1) # RECOMMENDED ONLY FOR FASTER TESTING\n",
    "\n",
    "    return model, dictionary, textBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define4\"></a>\n",
    "### 2.4 [function: package_mode](#TOC)\n",
    "Package the model, phraser and creation timestamp into a pickled (serialized) and gzip-ed object.\n",
    "\n",
    "Parameters:\n",
    "* model: The LDA model\n",
    "* phraser: The bigram phraser\n",
    "\n",
    "Results:\n",
    "* timestamp: The package creation timestamp\n",
    "* pkg_gz: The compressed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def package_model( model, phraser):\n",
    "    import pickle, gzip\n",
    "    from time import strftime\n",
    "\n",
    "    timestamp = strftime('%Y-%m-%d_%H.%M.%S')\n",
    "    pkg = { 'timestamp': timestamp,\n",
    "            'model': model,\n",
    "            'phraser': phraser\n",
    "          }\n",
    "    pkg_gz = gzip.compress(pickle.dumps(pkg))\n",
    "    \n",
    "    return timestamp, pkg_gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define5\"></a>\n",
    "### 2.5 [function: save_to_cos](#TOC)\n",
    "Save an object to Cloud Object Storage (COS) using REST.\n",
    "\n",
    "Parameters:\n",
    "* credentials: Dictionary of COS credentials.\n",
    "* bucket_name: COS bucket_name (must exist!)\n",
    "* object_name: Name of object to write.\n",
    "* data: Serialized data to write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_cos( credentials, bucket_name, object_name, data):\n",
    "    import requests\n",
    "\n",
    "    full_object_path = bucket_name + \"/\" + object_name\n",
    "    print(\"Saving\", full_object_path, \"(\", str(len(data)), \"bytes)\")\n",
    "\n",
    "    response = requests.post(\n",
    "                url = credentials['iam_url'],\n",
    "                headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "                params = {\"grant_type\":\"urn:ibm:params:oauth:grant-type:apikey\",\"apikey\":credentials['api_key']},\n",
    "                verify = True)\n",
    "    if str(response) != \"<Response [200]>\":\n",
    "        print( \"ERROR: POST Response =\", response)\n",
    "        return\n",
    "    bearer_token = response.json()[\"access_token\"]\n",
    "    \n",
    "    response = requests.put(\n",
    "                url = credentials['endpoint']+\"/\"+full_object_path,\n",
    "                headers = {\"Authorization\": \"bearer \" + bearer_token},\n",
    "                data = data)    \n",
    "    if str(response) != \"<Response [200]>\":\n",
    "        print( \"ERROR: PUT Response =\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 3. [Create the model](#TOC)\n",
    "\n",
    "* Download the NLTK collection of stop words.\n",
    "* Download the NLTK lemmatizer used in pre-processing the text.\n",
    "* Read a subset of the sample dataset to use for training.\n",
    "* Pre-process and cleanse the text data.\n",
    "* Train the LDA model.\n",
    "* Display some of the topic terms identified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Set logging level to display more messages (the gensim library provides many progress messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Set optional_logging to True to set logging level. Set optional_logging to False to ignore this.\n",
    "optional_logging = False\n",
    "if optional_logging:\n",
    "    import logging\n",
    "    #Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\n",
    "    logging.basicConfig( level=logging.ERROR, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel( logging.INFO)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the stoplist and lemmatizer from ntlk.download()\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stoplist = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "You may need to pip-install the gensim library. It is recommended to install version 2.2 as this is the version installed on the _Bluemix Streaming Analytics_ service at the time this notebook was tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --user gensim==2.2\n",
    "from gensim import models, corpora, utils\n",
    "\n",
    "# The 20-newsgroups dataset has 18846 entries. Let's take 5000 for training (this is not necessary a recommended ratio).\n",
    "texts = read_dataset(dataset + \".gz\", 5000)\n",
    "\n",
    "# Pre-process and cleanse the texts\n",
    "%time textTokens, bigram_phraser = preprocess_texts( texts, stoplist, lemmatizer)\n",
    "\n",
    "# train the model (save the dictionary and corpus for visualization)\n",
    "%time model, dictionary, corpus = train_model( textTokens)\n",
    "\n",
    "# Retrieve the topic terms from the model\n",
    "topicTerms = model.print_topics(num_topics=-1, num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display a sample (5) of the topic terms\n",
    "for tt in topicTerms[:5]:\n",
    "    print(\"Topic={0}, Terms={1}\".format(tt[0],tt[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the above command displays a topic number followed by a group of terms, where each term is coupled with its weight within the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualize\"></a>\n",
    "## 4. Display a visualization of the topic model\n",
    "We will use the pyLDAvis, which you can read about at http://pyldavis.readthedocs.io/en/latest/ and https://github.com/bmabey/pyLDAvis.  \n",
    "You may need to pip-install the pyLDAvis library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --user pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = pyLDAvis.gensim.prepare( model, corpus, dictionary)\n",
    "\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the interactive visualization to explore the dominant terms of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 5. Save the model and topic terms to Cloud Object Storage\n",
    "* Package the model and phraser into a dictionary object, which can be serialized and compressed to a file.\n",
    "* Save the file to COS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Package the model and phraser into a gzip object\n",
    "ts, pkg_gz = package_model( model, bigram_phraser)\n",
    "print(\"Package size: {} bytes; creation timestamp: {}\".format(len(pkg_gz), ts))\n",
    "\n",
    "# Stick the model creation timestamp into the name of the topic-terms file name\n",
    "topic_object_name_ts = topic_object_name.replace('.csv','') + '.' + ts + '.csv'\n",
    "print(\"Modified topic file name: {}\".format(topic_object_name_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write both files to COS\n",
    "save_to_cos( cos_credentials, model_bucket_name, model_object_name, pkg_gz)\n",
    "\n",
    "save_to_cos( cos_credentials, model_bucket_name, topic_object_name_ts, \n",
    "                '\\n'.join([str(t[0]) + \",\" + t[1] for t in topicTerms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps\n",
    "\n",
    "You have now created an LDA model and saved it to Cloud Object Storage.\n",
    "\n",
    "Proceed to part 2[LINK], to create a __Bluemix Streams flow for Streaming Analytics__, which uses the model to perform real-time topic prediction on news items from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"authors\"></a>Authors\n",
    "**Raanon Reutlinger** is a developer in the IBM Watson Data Platform team working on the __Streams Designer__ Bluemix (DSX) cloud service. Raanon has over 20 years experience in the IBM Software Group, working in Data Management, Big Data and Watson teams.\n",
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2017,2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
