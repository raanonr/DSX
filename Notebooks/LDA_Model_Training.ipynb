{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Introduction\nIn this notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Setup - Credentials and MH topic\nThe Cloud Object Storage (COS) credentials should NOT come from the service's \"Service credentials\" page.  \nGet them from the \"insert to code\" option in the DSX Notebook \"Data -> Connections\" panel, on the right.\nThen ADD an entry to cos_credentials for the service's PUBLIC endpoint.  \nSpecify the bucket and object names for the model which will be created and the list of topics identified.  \nThe bucket MUST already exist in the COS service.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# @hidden_cell\n\ncos_credentials = {\n  'iam_url':'<REPLACE>',\n  'api_key':'<REPLACE>',\n  'resource_instance_id':'<REPLACE>',\n  'url':'<REPLACE>',\n\n  'endpoint':'<REPLACE>'\n}\n\n# <REPLACE> these sample values if needed\nmodel_bucket_name = 'pyml'\nmodel_object_name = 'LDA_news.model.pickle'\ntopic_object_name = 'LDA_news.topic_terms.csv'", 
            "metadata": {}, 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "The Message Hub credentials should come from the service's \"Service credentials\" page.  \nSpecify your Message Hub topic name.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# @hidden_cell\n\nmh_credentials = {\n  \"kafka_brokers_sasl\": [\n      \"<REPLACE>\"\n  ],\n  \"user\": \"<REPLACE>\",\n  \"password\": \"<REPLACE>\"\n}\n\nmh_topic = '<REPLACE>'", 
            "metadata": {}, 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# RAANON\ncos_credentials_prod_ki = {\n  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n  'api_key':'0s-JWmaDBwiSd_yWJqenoKRBfTVU5Rgkz31CDT5WgoWQ',\n  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/db0d062d2b4c0836e18618a5222d8068:22e3b946-6154-4032-8e8f-7cfb0b429602::',\n  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n      \"endpoint\":\"https://s3-api.us-geo.objectstorage.softlayer.net\",\n}\ncos_credentials_stage1_wd = {\n  'iam_url':'https://iam.stage1.ng.bluemix.net/oidc/token',\n  'api_key':'xhjheSC7AhSLtvapSDnbyFn17uWUqW5ccAOuHhQxnnEY',\n  'resource_instance_id':'crn:v1:staging:public:cloud-object-storage:global:a/68a66698d275aeb48097f868957ab2ed:bbb5aa36-5525-4000-b129-bcb780195098::',\n  'url':'https://s3-api.us-geo.objectstorage.uat.service.networklayer.com',\n    'endpoint':'https://s3.us-west.objectstorage.uat.softlayer.net'\n}\n\nmh_credentials_stage1_2s = {\n  \"instance_id\": \"81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-stage1.messagehub.services.us-south.bluemix.net/Lookup?serviceId=81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"api_key\": \"phXq2H0NSDQNSCdKGJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\",\n  \"kafka_admin_url\": \"https://kafka-admin-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-stage1.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"phXq2H0NSDQNSCdK\",\n  \"password\": \"GJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\"\n}\n\n#cos_credentials = cos_credentials_prod_ki\ncos_credentials = cos_credentials_stage1_wd\nmh_credentials = mh_credentials_stage1_2s\n\nmh_topic = 'testTopic1'\nmodel_bucket_name = 'pyml'\nmodel_object_name = 'LDA_news.model.pickle'\ntopic_object_name = 'LDA_news.topic_terms.csv'", 
            "metadata": {}, 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Setup - Download helper functions and the dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We've provided a package of helper function. Download and import it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!rm -f watson_streaming_pipelines.py*\n!wget https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\n# You may need this:\n#!pip install kafka\n\nimport watson_streaming_pipelines as wstp", 
            "metadata": {}, 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-02 03:58:00--  https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11971 (12K) [text/plain]\nSaving to: \u2018watson_streaming_pipelines.py\u2019\n\n100%[======================================>] 11,971      --.-K/s   in 0s      \n\n2018-01-02 03:58:01 (34.8 MB/s) - \u2018watson_streaming_pipelines.py\u2019 saved [11971/11971]\n\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "#### The dataset\nVersion 3.2 of gensim (December 2017) includes a mechanism for downloading some sample datasets (see https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/ and https://radimrehurek.com/gensim/downloader.html).\nEven if you have a previous version of gensim, you can still download the sample dataset we'll be using with the following cell (based on the source code at https://github.com/RaRe-Technologies/gensim/blob/master/gensim/downloader.py).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "DOWNLOAD_BASE_URL = \"https://github.com/RaRe-Technologies/gensim-data/releases/download\"\ndataset=\"20-newsgroups\"\n\n!rm -f {dataset}.gz*\n!wget '{DOWNLOAD_BASE_URL}/{dataset}/{dataset}.gz'\n!ls -l {dataset}.gz*", 
            "metadata": {}, 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-02 03:58:07--  https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/20-newsgroups.gz\nResolving github.com (github.com)... 192.30.253.113, 192.30.253.112\nConnecting to github.com (github.com)|192.30.253.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180102T095807Z&X-Amz-Expires=300&X-Amz-Signature=1b7dc538b68da551ec59cd2ea077f4bc0f2cb367cf7381b9be4e8b7adbda755e&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream [following]\n--2018-01-02 03:58:07--  https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180102T095807Z&X-Amz-Expires=300&X-Amz-Signature=1b7dc538b68da551ec59cd2ea077f4bc0f2cb367cf7381b9be4e8b7adbda755e&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream\nResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.0.88\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.0.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14483581 (14M) [application/octet-stream]\nSaving to: \u201820-newsgroups.gz\u2019\n\n100%[======================================>] 14,483,581  7.91MB/s   in 1.7s   \n\n2018-01-02 03:58:10 (7.91 MB/s) - \u201820-newsgroups.gz\u2019 saved [14483581/14483581]\n\n-rw------- 1 sca9-7277eb31bca08b-bc196c953de3 users 14483581 Nov  9 17:44 20-newsgroups.gz\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### function: read_dataset\nLoad the dataset and create a List of texts.\n(All stored in memory, so assuming a small dataset.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def read_dataset(dataset_path, max_lines=0):\n    \"\"\"\n    Read the dataset and return a List of each 'data' entry.\n    \"\"\"\n    from smart_open import smart_open\n    import json\n\n    print(\"opening...\", dataset_path)\n    \n    data = []\n    with smart_open( dataset_path, 'rb') as infile:\n        for i, line in enumerate(infile):\n            if max_lines > 0 and i == max_lines:\n                break\n            jsonData = json.loads(line.decode('utf8'))\n            data.append(jsonData['data'])\n        infile.close()\n\n    print(len(data), \"lines read\")\n\n    return data", 
            "metadata": {}, 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: preprocess_texts\nSteps to pre-process and cleanse texts:\n1. Stopword Removal.\n2. Collocation detection (bigram).\n3. Lemmatization (not stem since stemming can reduce the interpretability).\n    \nParameters:\n* texts: List of texts.\n* stoplist: list of stopword tokens (from nltk.corpus.stopwords.words('english'))\n* lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer())\n    \nReturns:\n* tokens: Pre-processed tokenized texts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Adapted from https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/gensim_news_classification.ipynb\ndef preprocess_texts(texts, stoplist, lemmatizer=None):\n\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    tokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                     if word not in stoplist]\n               for text in texts]\n\n    # bigram collocation detection\n    bigram = models.Phrases(tokens)\n    bigram_phraser = models.phrases.Phraser(bigram)\n    tokens = [bigram_phraser[text] for text in tokens]\n\n    if lemmatizer:\n        tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n\n    return tokens", 
            "metadata": {}, 
            "execution_count": 7, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: train_model\nSteps to create the model\n1. Create a Dictionary using the List of cleansed tokenized text.\n2. [optional] Filter extremes.\n3. Create a corpus from the Bag-of-Words method.  \n    The BOW method takes the text tokens (words) and returns a list of tuples containing  \n    the word's token-id within the dictionary, and it's frequency within the input text.\n4. Create and train an LDA model. Play around with the hyperparameters to affect speed and quality.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def train_model( textTokens):\n\n    # Create the dictionary\n    dictionary = corpora.Dictionary( documents=textTokens)\n    \n    # Optional: Filter out tokens which are in less than 10 and more than 75.0% of the documents\n    dictionary.filter_extremes(no_below=10, no_above=0.75, keep_n=50000)\n\n    # The training corpus is the result of the Bag-of-Words method.\n    textBOW = [dictionary.doc2bow(text) for text in textTokens]\n\n    # Create the gensim LDA model - choose best arguments\n    model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dictionary,\n                                      num_topics=20, update_every=0.5,\n                                      # iterations=100, passes=3)\n                                      iterations=10, passes=1) # ONLY FOR FASTER TESTING\n\n    return model", 
            "metadata": {}, 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Begin work", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from gensim import models, corpora, utils\n###import importlib\n###importlib.reload(wstp)", 
            "metadata": {}, 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# Load the stoplist and lemmatizer from ntlk.download()\nstoplist = wstp.setStopWordList()\nlemmatizer = wstp.setLemmatizer()", 
            "metadata": {}, 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sca9-7277\n[nltk_data]     eb31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /gpfs/fs01/user/sca9-7277eb\n[nltk_data]     31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "## TEST", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# TEST\ntexts = read_dataset(dataset + \".gz\", 3500)\n# Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\ntokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\n           for text in texts]\n\n# bigram collocation detection\nbigram = models.Phrases(tokens)\nbigram_phraser = models.phrases.Phraser(bigram)\ntokens = [bigram_phraser[text] for text in tokens]\n\n#if lemmatizer:\n#    tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n", 
            "metadata": {}, 
            "execution_count": 41, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "opening... 20-newsgroups.gz\n3500 lines read\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# TEST\nflat = [b for a in tokens for b in a if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\nprint(len(flat))\nu = sorted(set(flat))\nprint(len(u))\nprint(u[:10])", 
            "metadata": {}, 
            "execution_count": 75, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "60618\n5191\n['a_costa', 'a_valid_return_e', 'aa_freenet', 'aamir_qazi', 'aantal_snijpunten', 'aardvark_ucs', 'aaron_bryce', 'aas_po', 'aau_dk', 'ab_h']\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# TEST\ntopiclist = model.show_topics(num_topics=-1, num_words=20, formatted=False)\nflat = [b[0] for a in topiclist for b in a[1] if \"_\" in b[0] and not b[0].startswith(\"_\") and not b[0].endswith(\"_\")]\n#u = sorted(set(flat))\nprint(*sorted(set(flat)), sep='\\n')", 
            "metadata": {}, 
            "execution_count": 150, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "baseball_players\nbrand_new\ndistribution_usa\nlines_article\nlines_nntp\nnntp_posting\norganization_university\nposting_host\nsale_organization\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# TEST\nimport logging\n#Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\nlogging.basicConfig( level=logging.INFO, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\nlogger = logging.getLogger()\nlogger.setLevel( logging.INFO)\n\ndef test1(tok):\n    print(tok)\n    flat = [b for b in tok if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\n    print(len(flat))\n    u = sorted(set(flat))\n    print(len(u))\n    print(u)\n\n# Find baseball\nfor i,d in enumerate(texts):\n    if \"baseball\" in d and \"players\" in d:\n        print(i) #, texts[i])\n        break\ntesttokens = [word for word in utils.tokenize(texts[2383], lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\nprint(testtokens)\nprint(\"=================\")\ntest1(bigram_phraser[testtokens])\n\ntestbigram = models.Phrases([testtokens], min_count=1, threshold=2)\ntestbigram_phraser = models.phrases.Phraser(testbigram)\nprint(\"=================\")\ntest1(testbigram_phraser[testtokens])\n\n### SO, the bigger bigram_phraser found more bigrams !!!\n", 
            "metadata": {}, 
            "execution_count": 190, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "text": "2018-01-02 12:28:54,681 : gensim.models.phrases.learn_vocab : INFO : collecting all words and their counts\n2018-01-02 12:28:54,682 : gensim.models.phrases.learn_vocab : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n2018-01-02 12:28:54,683 : gensim.models.phrases.learn_vocab : INFO : collected 122 word types from a corpus of 67 words (unigram + bigrams) and 1 sentences\n2018-01-02 12:28:54,684 : gensim.models.phrases.add_vocab : INFO : using 122 counts as vocab in Phrases<0 vocab, min_count=1, threshold=2, max_vocab_size=40000000>\n2018-01-02 12:28:54,685 : gensim.models.phrases.__init__ : INFO : source_vocab length 122\n2018-01-02 12:28:54,687 : gensim.models.phrases.__init__ : INFO : Phraser built with 3 3 phrasegrams\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stdout", 
                    "text": "2383\n['jlroffma', 'unix', 'amherst', 'edu', 'joshua', 'lawrence', 'roffman', 'subject', 'jewish', 'baseball', 'players', 'nntp', 'posting', 'host', 'amhux', 'amherst', 'edu', 'organization', 'amherst', 'college', 'x', 'newsreader', 'tin', 'version', 'pl', 'lines', 'baseball', 'players', 'past', 'present', 'able', 'come', 'much', 'except', 'sandy', 'koufax', 'somebody', 'stankowitz', 'maybe', 'john', 'lowenstein', 'anyone', 'come', 'know', 'sounds', 'pretty', 'lame', 'racking', 'brains', 'humor', 'us', 'thanks', 'help', 'john', 'lowenstein', 'definately', 'jewish', 'many', 'baltimore', 'thought', 'especially', 'told', 'baltimore', '_jewish', 'times_', 'later', 'admitted', 'joke']\n=================\n['jlroffma', 'unix_amherst', 'edu', 'joshua', 'lawrence', 'roffman', 'subject_jewish', 'baseball_players', 'nntp_posting', 'host', 'amhux', 'amherst_edu', 'organization', 'amherst_college', 'x_newsreader', 'tin_version', 'pl_lines', 'baseball_players', 'past_present', 'able_come', 'much_except', 'sandy_koufax', 'somebody_stankowitz', 'maybe_john', 'lowenstein_anyone', 'come', 'know_sounds', 'pretty_lame', 'racking_brains', 'humor_us', 'thanks_help', 'john_lowenstein', 'definately', 'jewish', 'many', 'baltimore', 'thought', 'especially', 'told', 'baltimore', '_jewish', 'times_', 'later', 'admitted', 'joke']\n23\n22\n['able_come', 'amherst_college', 'amherst_edu', 'baseball_players', 'humor_us', 'john_lowenstein', 'know_sounds', 'lowenstein_anyone', 'maybe_john', 'much_except', 'nntp_posting', 'past_present', 'pl_lines', 'pretty_lame', 'racking_brains', 'sandy_koufax', 'somebody_stankowitz', 'subject_jewish', 'thanks_help', 'tin_version', 'unix_amherst', 'x_newsreader']\n=================\n['jlroffma', 'unix', 'amherst_edu', 'joshua', 'lawrence', 'roffman', 'subject', 'jewish', 'baseball_players', 'nntp', 'posting', 'host', 'amhux', 'amherst_edu', 'organization', 'amherst', 'college', 'x', 'newsreader', 'tin', 'version', 'pl', 'lines', 'baseball_players', 'past', 'present', 'able', 'come', 'much', 'except', 'sandy', 'koufax', 'somebody', 'stankowitz', 'maybe', 'john_lowenstein', 'anyone', 'come', 'know', 'sounds', 'pretty', 'lame', 'racking', 'brains', 'humor', 'us', 'thanks', 'help', 'john_lowenstein', 'definately', 'jewish', 'many', 'baltimore', 'thought', 'especially', 'told', 'baltimore', '_jewish', 'times_', 'later', 'admitted', 'joke']\n6\n3\n['amherst_edu', 'baseball_players', 'john_lowenstein']\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### Train the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# The 20-newsgroups dataset has 18846 entries. Let's take 3500 for training.\ntexts = read_dataset(dataset + \".gz\", 3500)\n\n# Pre-process and cleanse the texts\ntextTokens = preprocess_texts( texts, stoplist, lemmatizer)\n\n# train the model\n%time model = train_model( textTokens)\n\n# Retrieve the topic terms from the model\ntopicTerms = model.print_topics(num_topics=-1, num_words=20)", 
            "metadata": {}, 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "opening... 20-newsgroups.gz\n3500 lines read\nCPU times: user 20.7 s, sys: 23.6 s, total: 44.3 s\nWall time: 23 s\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# Display a sample of the topic terms List\nfor tt in topicTerms[:5]:\n    print(\"Topic={0}, Terms={1}\".format(tt[0],tt[1]))", 
            "metadata": {}, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Topic=0, Terms=0.009*\"lines\" + 0.009*\"com\" + 0.009*\"organization\" + 0.006*\"edu\" + 0.005*\"sale\" + 0.005*\"writes\" + 0.004*\"one\" + 0.004*\"also\" + 0.004*\"would\" + 0.004*\"get\" + 0.004*\"nntp_posting\" + 0.004*\"good\" + 0.004*\"brand_new\" + 0.003*\"car\" + 0.003*\"know\" + 0.003*\"system\" + 0.003*\"looking\" + 0.003*\"new\" + 0.003*\"k\" + 0.003*\"think\"\nTopic=1, Terms=0.007*\"would\" + 0.006*\"one\" + 0.006*\"good\" + 0.006*\"organization\" + 0.005*\"edu\" + 0.005*\"writes\" + 0.005*\"era\" + 0.005*\"god\" + 0.004*\"get\" + 0.004*\"like\" + 0.004*\"may\" + 0.004*\"time\" + 0.004*\"lines\" + 0.004*\"people\" + 0.003*\"year\" + 0.003*\"think\" + 0.003*\"well\" + 0.003*\"players\" + 0.003*\"cubs\" + 0.003*\"first\"\nTopic=2, Terms=0.009*\"edu\" + 0.008*\"one\" + 0.007*\"would\" + 0.005*\"organization\" + 0.005*\"writes\" + 0.004*\"lines\" + 0.004*\"like\" + 0.004*\"baseball_players\" + 0.004*\"think\" + 0.004*\"also\" + 0.004*\"know\" + 0.004*\"people\" + 0.003*\"c\" + 0.003*\"good\" + 0.003*\"car\" + 0.003*\"team\" + 0.003*\"baseball\" + 0.003*\"well\" + 0.003*\"get\" + 0.003*\"much\"\nTopic=3, Terms=0.007*\"edu\" + 0.007*\"dos\" + 0.006*\"one\" + 0.005*\"god\" + 0.005*\"like\" + 0.005*\"organization\" + 0.005*\"would\" + 0.004*\"think\" + 0.004*\"know\" + 0.004*\"game\" + 0.003*\"get\" + 0.003*\"use\" + 0.003*\"also\" + 0.003*\"good\" + 0.003*\"lines\" + 0.003*\"may\" + 0.003*\"people\" + 0.003*\"back\" + 0.003*\"posting_host\" + 0.003*\"way\"\nTopic=4, Terms=0.008*\"edu\" + 0.007*\"organization\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"com\" + 0.006*\"writes\" + 0.006*\"think\" + 0.005*\"get\" + 0.004*\"like\" + 0.004*\"lines\" + 0.004*\"know\" + 0.004*\"time\" + 0.003*\"car\" + 0.003*\"also\" + 0.003*\"people\" + 0.003*\"host\" + 0.003*\"god\" + 0.003*\"well\" + 0.003*\"good\" + 0.003*\"nntp_posting\"\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### Save the model and topic terms to Cloud Object Storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "wstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + model_object_name, \n                wstp.pickleSerializer( data=model, zip=True))\n\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + topic_object_name, \n                '\\n'.join([str(t[0]) + \",\" + t[1] for t in topicTerms]))", 
            "metadata": {}, 
            "execution_count": 25, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Use the trained LDA model to identify the top topics for newsgroup texts", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Before continuing...\n#### Open Streams Designer\ndo this \nand that", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Stream the dataset texts to Message Hub", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Create the Message Hub producer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "producer = wstp.create_messagehub_producer( username = mh_credentials['user'], password = mh_credentials['password'], kafka_brokers_sasl = mh_credentials['kafka_brokers_sasl'])", 
            "metadata": {}, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### Send all of the text data to the MH topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import time\n\ndata = read_dataset(dataset + \".gz\")\nfor i, entry in enumerate(data):\n    producer.send( mh_topic, { 'text': entry } )\n    if ((i+1) % 1000) == 0:\n        print(i+1, end=\" \")\n        time.sleep(1) # Slow things down during demo", 
            "metadata": {}, 
            "execution_count": 40, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "opening... 20-newsgroups.gz\n18846 lines read\n1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 12000 13000 14000 15000 16000 17000 18000 ", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "python3-spark21", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "version": "3.5.2"
        }
    }
}