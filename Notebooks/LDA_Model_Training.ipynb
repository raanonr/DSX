{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Introduction\nIn this notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Setup - Credentials and MH topic\nThe Cloud Object Storage (COS) credentials should NOT come from the service's \"Service credentials\" page.  \nGet them from the \"insert to code\" option in the DSX Notebook \"Data -> Connections\" panel, on the right.\nThen ADD an entry to cos_credentials for the service's PUBLIC endpoint.  \nSpecify the bucket and object names for the model which will be created and the list of topics identified.  \nThe bucket MUST already exist in the COS service.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "# @hidden_cell\n\ncos_credentials = {\n  'iam_url':'<REPLACE>',\n  'api_key':'<REPLACE>',\n  'resource_instance_id':'<REPLACE>',\n  'url':'<REPLACE>',\n\n  'endpoint':'<REPLACE>'\n}\n\n# <REPLACE> these sample values if needed\nmodel_bucket_name = 'pyml'\nmodel_object_name = 'LDA_news.model.pkg.gz'\ntopic_object_name = 'LDA_news.topic_terms.csv'", 
            "execution_count": 1, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "The Message Hub credentials should come from the service's \"Service credentials\" page.  \nSpecify your Message Hub topic name.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "# @hidden_cell\n\nmh_credentials = {\n  \"kafka_brokers_sasl\": [\n      \"<REPLACE>\"\n  ],\n  \"user\": \"<REPLACE>\",\n  \"password\": \"<REPLACE>\"\n}\n\nmh_topic = '<REPLACE>'", 
            "execution_count": 2, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# RAANON\ncos_credentials_prod_ki = {\n  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n  'api_key':'0s-JWmaDBwiSd_yWJqenoKRBfTVU5Rgkz31CDT5WgoWQ',\n  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/db0d062d2b4c0836e18618a5222d8068:22e3b946-6154-4032-8e8f-7cfb0b429602::',\n  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n      \"endpoint\":\"https://s3-api.us-geo.objectstorage.softlayer.net\",\n}\ncos_credentials_stage1_wd = {\n  'iam_url':'https://iam.stage1.ng.bluemix.net/oidc/token',\n  'api_key':'xhjheSC7AhSLtvapSDnbyFn17uWUqW5ccAOuHhQxnnEY',\n  'resource_instance_id':'crn:v1:staging:public:cloud-object-storage:global:a/68a66698d275aeb48097f868957ab2ed:bbb5aa36-5525-4000-b129-bcb780195098::',\n  'url':'https://s3-api.us-geo.objectstorage.uat.service.networklayer.com',\n    'endpoint':'https://s3.us-west.objectstorage.uat.softlayer.net'\n}\n\nmh_credentials_stage1_2s = {\n  \"instance_id\": \"81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-stage1.messagehub.services.us-south.bluemix.net/Lookup?serviceId=81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"api_key\": \"phXq2H0NSDQNSCdKGJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\",\n  \"kafka_admin_url\": \"https://kafka-admin-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-stage1.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"phXq2H0NSDQNSCdK\",\n  \"password\": \"GJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\"\n}\n\n#cos_credentials = cos_credentials_prod_ki\ncos_credentials = cos_credentials_stage1_wd\nmh_credentials = mh_credentials_stage1_2s\n\nmh_topic = 'testTopic1'", 
            "execution_count": 3, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Setup - Download helper functions and the dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We've provided a package of helper function. Download and import it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "!rm -f watson_streaming_pipelines.py*\n!wget https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\n# You may need this:\n#!pip install kafka\n\nimport watson_streaming_pipelines as wstp", 
            "execution_count": 4, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-03 03:24:55--  https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11971 (12K) [text/plain]\nSaving to: \u2018watson_streaming_pipelines.py\u2019\n\n100%[======================================>] 11,971      --.-K/s   in 0.001s  \n\n2018-01-03 03:24:56 (13.7 MB/s) - \u2018watson_streaming_pipelines.py\u2019 saved [11971/11971]\n\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "#### The dataset\nVersion 3.2 of gensim (December 2017) includes a mechanism for downloading some sample datasets (see https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/ and https://radimrehurek.com/gensim/downloader.html).\nEven if you have a previous version of gensim, you can still download the sample dataset we'll be using with the following cell (based on the source code at https://github.com/RaRe-Technologies/gensim/blob/master/gensim/downloader.py).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "DOWNLOAD_BASE_URL = \"https://github.com/RaRe-Technologies/gensim-data/releases/download\"\ndataset=\"20-newsgroups\"\n\n!rm -f {dataset}.gz*\n!wget '{DOWNLOAD_BASE_URL}/{dataset}/{dataset}.gz'\n!ls -l {dataset}.gz*", 
            "execution_count": 5, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-03 03:25:14--  https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/20-newsgroups.gz\nResolving github.com (github.com)... 192.30.253.112, 192.30.253.113\nConnecting to github.com (github.com)|192.30.253.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180103T092515Z&X-Amz-Expires=300&X-Amz-Signature=125d8fee7d8be23805e226d7cb08b848c60dbde8fb3ae7a24468bd0a1406ef1c&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream [following]\n--2018-01-03 03:25:15--  https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180103T092515Z&X-Amz-Expires=300&X-Amz-Signature=125d8fee7d8be23805e226d7cb08b848c60dbde8fb3ae7a24468bd0a1406ef1c&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream\nResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.96.155\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.96.155|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14483581 (14M) [application/octet-stream]\nSaving to: \u201820-newsgroups.gz\u2019\n\n100%[======================================>] 14,483,581  9.04MB/s   in 1.5s   \n\n2018-01-03 03:25:16 (9.04 MB/s) - \u201820-newsgroups.gz\u2019 saved [14483581/14483581]\n\n-rw------- 1 sca9-7277eb31bca08b-bc196c953de3 users 14483581 Nov  9 17:44 20-newsgroups.gz\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### function: read_dataset\nLoad the dataset and create a List of texts.\n(All stored in memory, so assuming a small dataset.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "def read_dataset(dataset_path, max_lines=0):\n    \"\"\"\n    Read the dataset and return a List of each 'data' entry.\n    \"\"\"\n    from smart_open import smart_open\n    import json\n\n    print(\"opening...\", dataset_path)\n    \n    data = []\n    with smart_open( dataset_path, 'rb') as infile:\n        for i, line in enumerate(infile):\n            if max_lines > 0 and i == max_lines:\n                break\n            jsonData = json.loads(line.decode('utf8'))\n            data.append(jsonData['data'])\n        infile.close()\n\n    print(len(data), \"lines read\")\n\n    return data", 
            "execution_count": 6, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: preprocess_texts\nSteps to pre-process and cleanse texts:\n1. Stopword Removal.\n2. Collocation detection (bigram).\n3. Lemmatization (not stem since stemming can reduce the interpretability).\n    \nParameters:\n* texts: List of texts.\n* stoplist: list of stopword tokens (from nltk.corpus.stopwords.words('english'))\n* lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer())\n    \nReturns:\n* tokens: Pre-processed tokenized texts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "# Adapted from https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/gensim_news_classification.ipynb\ndef preprocess_texts(texts, stoplist, lemmatizer=None):\n\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    tokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                     if word not in stoplist]\n               for text in texts]\n\n    # bigram collocation detection\n    bigram = models.Phrases(tokens)\n    bigram_phraser = models.phrases.Phraser(bigram)\n    tokens = [bigram_phraser[text] for text in tokens]\n\n    if lemmatizer:\n        tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n\n    return tokens, bigram_phraser", 
            "execution_count": 20, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: train_model\nSteps to create the model\n1. Create a Dictionary using the List of cleansed tokenized text.\n2. [optional] Filter extremes.\n3. Create a corpus from the Bag-of-Words method.  \n    The BOW method takes the text tokens (words) and returns a list of tuples containing  \n    the word's token-id within the dictionary, and it's frequency within the input text.\n4. Create and train an LDA model. Play around with the hyperparameters to affect speed and quality.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "def train_model( textTokens):\n\n    # Create the dictionary\n    dictionary = corpora.Dictionary( documents=textTokens)\n    \n    # Optional: Filter out tokens which are in less than 10 and more than 75.0% of the documents\n    dictionary.filter_extremes(no_below=10, no_above=0.75, keep_n=50000)\n\n    # The training corpus is the result of the Bag-of-Words method.\n    textBOW = [dictionary.doc2bow(text) for text in textTokens]\n\n    # Create the gensim LDA model - choose best arguments\n    model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dictionary,\n                                      num_topics=20, update_every=0.5,\n                                      # iterations=100, passes=3)\n                                      iterations=10, passes=1) # ONLY FOR FASTER TESTING\n\n    return model", 
            "execution_count": 8, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "def package_model( model, phraser):\n    import pickle, gzip\n    from time import strftime\n\n    timestamp = strftime('%Y-%m-%d_%H.%M.%S')\n    pkg = { 'timestamp': timestamp,\n            'model': model,\n            'phraser': phraser\n          }\n    pkg_gz = gzip.compress(pickle.dumps(pkg))\n    \n    return timestamp, pkg_gz", 
            "execution_count": 9, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Begin work", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "from gensim import models, corpora, utils\n###import importlib\n###importlib.reload(wstp)", 
            "execution_count": 10, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "text": "Using TensorFlow backend.\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "# Optional: Set optional_logging to True to set logging level. Set optional_logging to False to ignore this.\noptional_logging = True\nif optional_logging:\n    import logging, warnings\n    #Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\n    logging.basicConfig( level=logging.ERROR, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\n    logger = logging.getLogger()\n    logger.setLevel( logging.INFO)\n    wstp.setLogLevel( logging.INFO)\n    warnings.simplefilter('ignore')", 
            "execution_count": 31, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# Load the stoplist and lemmatizer from ntlk.download()\nstoplist = wstp.setStopWordList()\nlemmatizer = wstp.setLemmatizer()", 
            "execution_count": 11, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sca9-7277\n[nltk_data]     eb31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /gpfs/fs01/user/sca9-7277eb\n[nltk_data]     31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "## TEST", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "# TEST\ntexts = read_dataset(dataset + \".gz\", 3500)\n# Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\ntokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\n           for text in texts]\n\n# bigram collocation detection\nbigram = models.Phrases(tokens)\nbigram_phraser = models.phrases.Phraser(bigram)\ntokens = [bigram_phraser[text] for text in tokens]\n\n#if lemmatizer:\n#    tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# TEST\nflat = [b for a in tokens for b in a if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\nprint(len(flat))\nu = sorted(set(flat))\nprint(len(u))\nprint(u[:10])", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# TEST\ntopiclist = model.show_topics(num_topics=-1, num_words=20, formatted=False)\nflat = [b[0] for a in topiclist for b in a[1] if \"_\" in b[0] and not b[0].startswith(\"_\") and not b[0].endswith(\"_\")]\n#u = sorted(set(flat))\nprint(*sorted(set(flat)), sep='\\n')", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "# TEST\nimport logging\n#Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\nlogging.basicConfig( level=logging.INFO, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\nlogger = logging.getLogger()\nlogger.setLevel( logging.INFO)\n\ndef test1(tok):\n    print(tok)\n    flat = [b for b in tok if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\n    print(len(flat))\n    u = sorted(set(flat))\n    print(len(u))\n    print(u)\n\n# Find baseball\nfor i,d in enumerate(texts):\n    if \"baseball\" in d and \"players\" in d:\n        print(i) #, texts[i])\n        break\ntesttokens = [word for word in utils.tokenize(texts[2383], lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\nprint(testtokens)\nprint(\"=================\")\ntest1(bigram_phraser[testtokens])\n\ntestbigram = models.Phrases([testtokens], min_count=1, threshold=2)\ntestbigram_phraser = models.phrases.Phraser(testbigram)\nprint(\"=================\")\ntest1(testbigram_phraser[testtokens])\n\n### SO, the bigger bigram_phraser found more bigrams !!!\n", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### Train the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "# The 20-newsgroups dataset has 18846 entries. Let's take 3500 for training.\ntexts = read_dataset(dataset + \".gz\", 3500)\n\n# Pre-process and cleanse the texts\n%time textTokens, bigram_phraser = preprocess_texts( texts, stoplist, lemmatizer)\n\n# train the model\n%time model = train_model( textTokens)\n\n# Retrieve the topic terms from the model\ntopicTerms = model.print_topics(num_topics=-1, num_words=20)", 
            "execution_count": 22, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "opening... 20-newsgroups.gz\n3500 lines read\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stderr", 
                    "text": "2018-01-03 03:50:23,985 : gensim.models.phrases.learn_vocab : INFO : collecting all words and their counts\n2018-01-03 03:50:23,987 : gensim.models.phrases.learn_vocab : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n2018-01-03 03:50:24,970 : gensim.models.phrases.learn_vocab : INFO : collected 316160 word types from a corpus of 485115 words (unigram + bigrams) and 3500 sentences\n2018-01-03 03:50:24,971 : gensim.models.phrases.add_vocab : INFO : using 316160 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n2018-01-03 03:50:24,972 : gensim.models.phrases.__init__ : INFO : source_vocab length 316160\n2018-01-03 03:50:27,807 : gensim.models.phrases.__init__ : INFO : Phraser built with 6664 6664 phrasegrams\n2018-01-03 03:50:29,187 : gensim.corpora.dictionary.add_documents : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stdout", 
                    "text": "CPU times: user 8.12 s, sys: 20.2 ms, total: 8.14 s\nWall time: 8.14 s\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stderr", 
                    "text": "2018-01-03 03:50:29,828 : gensim.corpora.dictionary.add_documents : INFO : built Dictionary(40114 unique tokens: ['anybody_knows', 'comiskey', 'bprisco', 'mobile', 'sourgas']...) from 3500 documents (total 427557 corpus positions)\n2018-01-03 03:50:29,894 : gensim.corpora.dictionary.filter_extremes : INFO : discarding 34202 tokens: [('prevelant', 1), ('barred', 2), ('mete', 2), ('guarantees', 3), ('resolved', 7), ('purgatory', 3), ('slaughter', 3), ('militarily', 1), ('calvinists', 1), ('gommorrah', 1)]...\n2018-01-03 03:50:29,895 : gensim.corpora.dictionary.filter_extremes : INFO : keeping 5912 tokens which were in no less than 10 and no more than 2625 (=75.0%) documents\n2018-01-03 03:50:29,912 : gensim.corpora.dictionary.filter_extremes : INFO : resulting dictionary: Dictionary(5912 unique tokens: ['mad', 'mobile', 'early_christians', 'circle', 'volunteer']...)\n2018-01-03 03:50:30,337 : gensim.models.ldamodel.init_dir_prior : INFO : using symmetric alpha at 0.05\n2018-01-03 03:50:30,338 : gensim.models.ldamodel.init_dir_prior : INFO : using symmetric eta at 0.00016914749661705008\n2018-01-03 03:50:30,340 : gensim.models.ldamodel.__init__ : INFO : using serial LDA version on this node\n2018-01-03 03:50:32,019 : gensim.models.ldamodel.update : INFO : running online (single-pass) LDA training, 20 topics, 1 passes over the supplied corpus of 3500 documents, updating model once every 1000 documents, evaluating perplexity every 3500 documents, iterating 10x with a convergence threshold of 0.001000\n2018-01-03 03:50:32,020 : gensim.models.ldamodel.update : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n2018-01-03 03:50:32,021 : gensim.models.ldamodel.update : INFO : PROGRESS: pass 0, at document #2000/3500\n2018-01-03 03:50:37,074 : gensim.models.ldamodel.blend : INFO : merging changes from 2000 documents into a model of 3500 documents\n2018-01-03 03:50:38,347 : gensim.models.ldamodel.show_topics : INFO : topic #12 (0.050): 0.007*\"god\" + 0.006*\"one\" + 0.006*\"organization\" + 0.005*\"writes\" + 0.005*\"would\" + 0.004*\"edu\" + 0.004*\"com\" + 0.004*\"also\" + 0.004*\"good\" + 0.004*\"think\"\n2018-01-03 03:50:38,348 : gensim.models.ldamodel.show_topics : INFO : topic #13 (0.050): 0.010*\"edu\" + 0.007*\"organization\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"god\" + 0.004*\"think\" + 0.004*\"lines\" + 0.004*\"com\" + 0.004*\"people\" + 0.004*\"us\"\n2018-01-03 03:50:38,349 : gensim.models.ldamodel.show_topics : INFO : topic #4 (0.050): 0.008*\"would\" + 0.007*\"one\" + 0.006*\"edu\" + 0.005*\"also\" + 0.005*\"think\" + 0.005*\"god\" + 0.004*\"people\" + 0.004*\"writes\" + 0.004*\"organization\" + 0.004*\"get\"\n2018-01-03 03:50:38,350 : gensim.models.ldamodel.show_topics : INFO : topic #10 (0.050): 0.008*\"god\" + 0.007*\"edu\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"organization\" + 0.005*\"people\" + 0.005*\"writes\" + 0.005*\"lines\" + 0.004*\"get\" + 0.004*\"bike\"\n2018-01-03 03:50:38,351 : gensim.models.ldamodel.show_topics : INFO : topic #14 (0.050): 0.007*\"organization\" + 0.007*\"god\" + 0.006*\"one\" + 0.006*\"writes\" + 0.005*\"edu\" + 0.005*\"would\" + 0.004*\"get\" + 0.004*\"know\" + 0.004*\"think\" + 0.004*\"lines\"\n2018-01-03 03:50:38,352 : gensim.models.ldamodel.do_mstep : INFO : topic diff=2.194338, rho=1.000000\n2018-01-03 03:50:47,689 : gensim.models.ldamodel.log_perplexity : INFO : -10.778 per-word bound, 1756.2 perplexity estimate based on a held-out corpus of 1500 documents with 114161 words\n2018-01-03 03:50:47,690 : gensim.models.ldamodel.update : INFO : PROGRESS: pass 0, at document #3500/3500\n2018-01-03 03:50:50,744 : gensim.models.ldamodel.blend : INFO : merging changes from 1500 documents into a model of 3500 documents\n2018-01-03 03:50:51,727 : gensim.models.ldamodel.show_topics : INFO : topic #2 (0.050): 0.010*\"organization\" + 0.008*\"one\" + 0.008*\"com\" + 0.006*\"would\" + 0.006*\"edu\" + 0.006*\"cubs\" + 0.005*\"lines\" + 0.005*\"good\" + 0.005*\"writes\" + 0.005*\"also\"\n2018-01-03 03:50:51,728 : gensim.models.ldamodel.show_topics : INFO : topic #4 (0.050): 0.008*\"would\" + 0.006*\"one\" + 0.006*\"era\" + 0.006*\"edu\" + 0.005*\"com\" + 0.005*\"think\" + 0.005*\"car\" + 0.004*\"writes\" + 0.004*\"get\" + 0.004*\"also\"\n2018-01-03 03:50:51,729 : gensim.models.ldamodel.show_topics : INFO : topic #7 (0.050): 0.008*\"one\" + 0.008*\"edu\" + 0.007*\"organization\" + 0.006*\"think\" + 0.006*\"lines\" + 0.005*\"com\" + 0.005*\"car\" + 0.005*\"nntp_posting\" + 0.004*\"like\" + 0.004*\"writes\"\n2018-01-03 03:50:51,730 : gensim.models.ldamodel.show_topics : INFO : topic #9 (0.050): 0.011*\"edu\" + 0.008*\"com\" + 0.008*\"one\" + 0.006*\"organization\" + 0.006*\"writes\" + 0.005*\"nntp_posting\" + 0.005*\"get\" + 0.005*\"lines\" + 0.004*\"know\" + 0.004*\"morris\"\n2018-01-03 03:50:51,731 : gensim.models.ldamodel.show_topics : INFO : topic #11 (0.050): 0.012*\"edu\" + 0.007*\"organization\" + 0.006*\"would\" + 0.005*\"lines\" + 0.005*\"writes\" + 0.005*\"one\" + 0.004*\"teams\" + 0.004*\"nntp_posting\" + 0.004*\"car\" + 0.004*\"well\"\n2018-01-03 03:50:51,732 : gensim.models.ldamodel.do_mstep : INFO : topic diff=1.918283, rho=0.707107\n2018-01-03 03:50:51,747 : gensim.models.ldamodel.show_topics : INFO : topic #0 (0.050): 0.007*\"com\" + 0.007*\"edu\" + 0.006*\"organization\" + 0.006*\"one\" + 0.005*\"lines\" + 0.005*\"writes\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"get\" + 0.004*\"good\" + 0.004*\"car\" + 0.004*\"think\" + 0.004*\"time\" + 0.004*\"integra\" + 0.003*\"know\" + 0.003*\"like\" + 0.003*\"aaa\" + 0.003*\"c\" + 0.003*\"also\" + 0.003*\"much\"\n2018-01-03 03:50:51,748 : gensim.models.ldamodel.show_topics : INFO : topic #1 (0.050): 0.010*\"braves\" + 0.006*\"edu\" + 0.005*\"one\" + 0.005*\"like\" + 0.005*\"would\" + 0.004*\"com\" + 0.004*\"organization\" + 0.004*\"car\" + 0.004*\"writes\" + 0.004*\"players\" + 0.004*\"lines\" + 0.004*\"get\" + 0.004*\"well\" + 0.004*\"know\" + 0.004*\"people\" + 0.004*\"think\" + 0.003*\"sale_organization\" + 0.003*\"good\" + 0.003*\"wins\" + 0.003*\"lines_article\"\n2018-01-03 03:50:51,749 : gensim.models.ldamodel.show_topics : INFO : topic #2 (0.050): 0.010*\"organization\" + 0.008*\"one\" + 0.008*\"com\" + 0.006*\"would\" + 0.006*\"edu\" + 0.006*\"cubs\" + 0.005*\"lines\" + 0.005*\"good\" + 0.005*\"writes\" + 0.005*\"also\" + 0.004*\"get\" + 0.004*\"nntp_posting\" + 0.004*\"c\" + 0.004*\"season\" + 0.004*\"time\" + 0.004*\"know\" + 0.003*\"see\" + 0.003*\"think\" + 0.003*\"go\" + 0.003*\"use\"\n2018-01-03 03:50:51,750 : gensim.models.ldamodel.show_topics : INFO : topic #3 (0.050): 0.007*\"would\" + 0.007*\"edu\" + 0.006*\"one\" + 0.006*\"organization\" + 0.005*\"think\" + 0.004*\"lines\" + 0.004*\"like\" + 0.004*\"good\" + 0.004*\"car\" + 0.004*\"writes\" + 0.004*\"also\" + 0.004*\"com\" + 0.004*\"know\" + 0.003*\"first\" + 0.003*\"get\" + 0.003*\"time\" + 0.003*\"x\" + 0.003*\"sale\" + 0.003*\"see\" + 0.003*\"even\"\n2018-01-03 03:50:51,751 : gensim.models.ldamodel.show_topics : INFO : topic #4 (0.050): 0.008*\"would\" + 0.006*\"one\" + 0.006*\"era\" + 0.006*\"edu\" + 0.005*\"com\" + 0.005*\"think\" + 0.005*\"car\" + 0.004*\"writes\" + 0.004*\"get\" + 0.004*\"also\" + 0.004*\"may\" + 0.004*\"organization\" + 0.004*\"go\" + 0.003*\"people\" + 0.003*\"good\" + 0.003*\"lines\" + 0.003*\"know\" + 0.003*\"baseball\" + 0.003*\"article\" + 0.003*\"time\"\n2018-01-03 03:50:51,752 : gensim.models.ldamodel.show_topics : INFO : topic #5 (0.050): 0.011*\"edu\" + 0.008*\"one\" + 0.007*\"would\" + 0.007*\"com\" + 0.006*\"lines\" + 0.005*\"get\" + 0.005*\"organization\" + 0.005*\"know\" + 0.004*\"writes\" + 0.004*\"like\" + 0.004*\"also\" + 0.004*\"good\" + 0.004*\"car\" + 0.003*\"article\" + 0.003*\"new\" + 0.003*\"could\" + 0.003*\"game\" + 0.003*\"time\" + 0.003*\"problem\" + 0.003*\"___\"\n2018-01-03 03:50:51,753 : gensim.models.ldamodel.show_topics : INFO : topic #6 (0.050): 0.009*\"edu\" + 0.008*\"one\" + 0.006*\"organization\" + 0.006*\"com\" + 0.005*\"lines\" + 0.005*\"car\" + 0.005*\"think\" + 0.005*\"good\" + 0.005*\"would\" + 0.005*\"also\" + 0.005*\"new\" + 0.003*\"sale\" + 0.003*\"game\" + 0.003*\"two\" + 0.003*\"right\" + 0.003*\"like\" + 0.003*\"writes\" + 0.003*\"god\" + 0.003*\"reply\" + 0.003*\"way\"\n2018-01-03 03:50:51,754 : gensim.models.ldamodel.show_topics : INFO : topic #7 (0.050): 0.008*\"one\" + 0.008*\"edu\" + 0.007*\"organization\" + 0.006*\"think\" + 0.006*\"lines\" + 0.005*\"com\" + 0.005*\"car\" + 0.005*\"nntp_posting\" + 0.004*\"like\" + 0.004*\"writes\" + 0.004*\"get\" + 0.004*\"host\" + 0.003*\"game\" + 0.003*\"new\" + 0.003*\"used\" + 0.003*\"time\" + 0.003*\"good\" + 0.003*\"c\" + 0.003*\"go\" + 0.003*\"got\"\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stderr", 
                    "text": "2018-01-03 03:50:51,755 : gensim.models.ldamodel.show_topics : INFO : topic #8 (0.050): 0.017*\"dos\" + 0.009*\"edu\" + 0.006*\"would\" + 0.005*\"writes\" + 0.005*\"organization\" + 0.005*\"one\" + 0.004*\"lines\" + 0.004*\"com\" + 0.004*\"know\" + 0.004*\"good\" + 0.004*\"like\" + 0.004*\"time\" + 0.003*\"god\" + 0.003*\"game\" + 0.003*\"also\" + 0.003*\"cs_cornell\" + 0.003*\"car\" + 0.003*\"us\" + 0.003*\"get\" + 0.003*\"rockies\"\n2018-01-03 03:50:51,756 : gensim.models.ldamodel.show_topics : INFO : topic #9 (0.050): 0.011*\"edu\" + 0.008*\"com\" + 0.008*\"one\" + 0.006*\"organization\" + 0.006*\"writes\" + 0.005*\"nntp_posting\" + 0.005*\"get\" + 0.005*\"lines\" + 0.004*\"know\" + 0.004*\"morris\" + 0.004*\"host\" + 0.004*\"sale\" + 0.004*\"new\" + 0.004*\"also\" + 0.004*\"would\" + 0.003*\"car\" + 0.003*\"good\" + 0.003*\"game\" + 0.003*\"think\" + 0.003*\"like\"\n2018-01-03 03:50:51,757 : gensim.models.ldamodel.show_topics : INFO : topic #10 (0.050): 0.013*\"edu\" + 0.006*\"organization\" + 0.006*\"one\" + 0.006*\"would\" + 0.005*\"car\" + 0.005*\"writes\" + 0.005*\"lines\" + 0.005*\"nntp_posting\" + 0.004*\"posting_host\" + 0.004*\"good\" + 0.004*\"organization_university\" + 0.004*\"get\" + 0.003*\"host\" + 0.003*\"com\" + 0.003*\"also\" + 0.003*\"baseball_players\" + 0.003*\"league\" + 0.003*\"like\" + 0.003*\"people\" + 0.003*\"know\"\n2018-01-03 03:50:51,758 : gensim.models.ldamodel.show_topics : INFO : topic #11 (0.050): 0.012*\"edu\" + 0.007*\"organization\" + 0.006*\"would\" + 0.005*\"lines\" + 0.005*\"writes\" + 0.005*\"one\" + 0.004*\"teams\" + 0.004*\"nntp_posting\" + 0.004*\"car\" + 0.004*\"well\" + 0.003*\"new\" + 0.003*\"mets\" + 0.003*\"good\" + 0.003*\"team\" + 0.003*\"also\" + 0.003*\"like\" + 0.003*\"host\" + 0.003*\"posting_host\" + 0.003*\"sale\" + 0.003*\"players\"\n2018-01-03 03:50:51,759 : gensim.models.ldamodel.show_topics : INFO : topic #12 (0.050): 0.006*\"organization\" + 0.006*\"one\" + 0.006*\"good\" + 0.004*\"com\" + 0.004*\"edu\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"career\" + 0.003*\"like\" + 0.003*\"also\" + 0.003*\"may\" + 0.003*\"league\" + 0.003*\"think\" + 0.003*\"obo\" + 0.003*\"well\" + 0.003*\"god\" + 0.003*\"know\" + 0.003*\"car\" + 0.003*\"lines\" + 0.003*\"people\"\n2018-01-03 03:50:51,760 : gensim.models.ldamodel.show_topics : INFO : topic #13 (0.050): 0.017*\"edu\" + 0.008*\"organization\" + 0.006*\"lopez\" + 0.006*\"would\" + 0.006*\"one\" + 0.005*\"lines\" + 0.005*\"car\" + 0.004*\"think\" + 0.004*\"host\" + 0.004*\"writes\" + 0.004*\"nntp_posting\" + 0.003*\"com\" + 0.003*\"time\" + 0.003*\"even\" + 0.003*\"see\" + 0.003*\"know\" + 0.003*\"good\" + 0.003*\"well\" + 0.003*\"also\" + 0.003*\"use\"\n2018-01-03 03:50:51,761 : gensim.models.ldamodel.show_topics : INFO : topic #14 (0.050): 0.007*\"edu\" + 0.007*\"organization\" + 0.006*\"car\" + 0.006*\"one\" + 0.006*\"writes\" + 0.004*\"get\" + 0.004*\"would\" + 0.004*\"lines\" + 0.004*\"think\" + 0.004*\"like\" + 0.004*\"well\" + 0.003*\"year\" + 0.003*\"know\" + 0.003*\"may\" + 0.003*\"com\" + 0.003*\"posting_host\" + 0.003*\"lines_nntp\" + 0.003*\"c\" + 0.003*\"also\" + 0.003*\"cars\"\n2018-01-03 03:50:51,762 : gensim.models.ldamodel.show_topics : INFO : topic #15 (0.050): 0.014*\"edu\" + 0.007*\"one\" + 0.005*\"would\" + 0.005*\"organization\" + 0.004*\"also\" + 0.004*\"good\" + 0.004*\"writes\" + 0.004*\"lines\" + 0.004*\"people\" + 0.003*\"think\" + 0.003*\"god\" + 0.003*\"com\" + 0.003*\"like\" + 0.003*\"get\" + 0.003*\"game\" + 0.003*\"time\" + 0.003*\"even\" + 0.003*\"know\" + 0.003*\"system\" + 0.003*\"us\"\n2018-01-03 03:50:51,763 : gensim.models.ldamodel.show_topics : INFO : topic #16 (0.050): 0.006*\"edu\" + 0.006*\"one\" + 0.006*\"good\" + 0.006*\"would\" + 0.005*\"organization\" + 0.005*\"get\" + 0.004*\"god\" + 0.004*\"lines\" + 0.004*\"may\" + 0.004*\"com\" + 0.004*\"time\" + 0.004*\"writes\" + 0.004*\"like\" + 0.004*\"people\" + 0.004*\"car\" + 0.004*\"also\" + 0.004*\"know\" + 0.004*\"think\" + 0.003*\"use\" + 0.003*\"could\"\n2018-01-03 03:50:51,764 : gensim.models.ldamodel.show_topics : INFO : topic #17 (0.050): 0.008*\"one\" + 0.007*\"com\" + 0.007*\"organization\" + 0.005*\"edu\" + 0.005*\"writes\" + 0.005*\"lines\" + 0.005*\"like\" + 0.005*\"may\" + 0.005*\"think\" + 0.004*\"would\" + 0.004*\"reply\" + 0.004*\"also\" + 0.004*\"well\" + 0.003*\"good\" + 0.003*\"way\" + 0.003*\"nntp_posting\" + 0.003*\"get\" + 0.003*\"baseball\" + 0.003*\"organization_university\" + 0.003*\"c\"\n2018-01-03 03:50:51,765 : gensim.models.ldamodel.show_topics : INFO : topic #18 (0.050): 0.012*\"edu\" + 0.011*\"baseball\" + 0.007*\"one\" + 0.007*\"would\" + 0.006*\"writes\" + 0.005*\"organization\" + 0.005*\"get\" + 0.004*\"good\" + 0.004*\"know\" + 0.004*\"think\" + 0.004*\"nntp_posting\" + 0.004*\"lines\" + 0.004*\"like\" + 0.003*\"people\" + 0.003*\"well\" + 0.003*\"lines_article\" + 0.003*\"also\" + 0.003*\"c\" + 0.003*\"apr\" + 0.003*\"com\"\n2018-01-03 03:50:51,766 : gensim.models.ldamodel.show_topics : INFO : topic #19 (0.050): 0.007*\"would\" + 0.007*\"edu\" + 0.006*\"lines\" + 0.005*\"one\" + 0.005*\"car\" + 0.005*\"organization\" + 0.005*\"com\" + 0.005*\"think\" + 0.005*\"writes\" + 0.004*\"like\" + 0.004*\"people\" + 0.003*\"know\" + 0.003*\"team\" + 0.003*\"also\" + 0.003*\"get\" + 0.003*\"new\" + 0.003*\"time\" + 0.003*\"may\" + 0.003*\"even\" + 0.003*\"since\"\n", 
                    "output_type": "stream"
                }, 
                {
                    "name": "stdout", 
                    "text": "CPU times: user 19.8 s, sys: 26.2 s, total: 46 s\nWall time: 22.6 s\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "# Display a sample of the topic terms List\nfor tt in topicTerms[:5]:\n    print(\"Topic={0}, Terms={1}\".format(tt[0],tt[1]))", 
            "execution_count": 18, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "Topic=0, Terms=0.009*\"com\" + 0.009*\"organization\" + 0.009*\"one\" + 0.006*\"lines\" + 0.005*\"car\" + 0.005*\"edu\" + 0.005*\"also\" + 0.005*\"would\" + 0.004*\"writes\" + 0.004*\"get\" + 0.004*\"make_offer\" + 0.004*\"us\" + 0.003*\"r\" + 0.003*\"time\" + 0.003*\"sale\" + 0.003*\"like\" + 0.003*\"see\" + 0.003*\"new\" + 0.003*\"think\" + 0.003*\"god\"\nTopic=1, Terms=0.010*\"baseball\" + 0.006*\"would\" + 0.006*\"one\" + 0.005*\"writes\" + 0.005*\"league\" + 0.005*\"know\" + 0.005*\"good\" + 0.004*\"edu\" + 0.004*\"get\" + 0.004*\"new\" + 0.004*\"like\" + 0.004*\"car\" + 0.004*\"think\" + 0.003*\"organization\" + 0.003*\"much\" + 0.003*\"fan\" + 0.003*\"com\" + 0.003*\"time\" + 0.003*\"well\" + 0.003*\"stadium\"\nTopic=2, Terms=0.010*\"edu\" + 0.007*\"would\" + 0.006*\"car\" + 0.006*\"organization\" + 0.005*\"writes\" + 0.005*\"good\" + 0.004*\"one\" + 0.004*\"think\" + 0.004*\"sale\" + 0.004*\"c\" + 0.004*\"people\" + 0.004*\"year\" + 0.004*\"com\" + 0.003*\"know\" + 0.003*\"time\" + 0.003*\"well\" + 0.003*\"like\" + 0.003*\"also\" + 0.003*\"lines\" + 0.003*\"two\"\nTopic=3, Terms=0.009*\"one\" + 0.005*\"edu\" + 0.005*\"writes\" + 0.005*\"lines\" + 0.005*\"com\" + 0.004*\"see\" + 0.004*\"get\" + 0.004*\"time\" + 0.004*\"car\" + 0.004*\"also\" + 0.004*\"may\" + 0.004*\"new\" + 0.004*\"even\" + 0.004*\"would\" + 0.003*\"well\" + 0.003*\"know\" + 0.003*\"like\" + 0.003*\"lines_article\" + 0.003*\"organization\" + 0.003*\"think\"\nTopic=4, Terms=0.007*\"organization\" + 0.006*\"one\" + 0.006*\"edu\" + 0.005*\"new\" + 0.005*\"lines\" + 0.004*\"know\" + 0.004*\"think\" + 0.004*\"get\" + 0.004*\"writes\" + 0.004*\"would\" + 0.004*\"people\" + 0.003*\"good\" + 0.003*\"lens\" + 0.003*\"dos\" + 0.003*\"god\" + 0.003*\"even\" + 0.003*\"time\" + 0.003*\"like\" + 0.003*\"also\" + 0.003*\"com\"\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### Save the model and topic terms to Cloud Object Storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "ts, pkg_gz = package_model( model, bigram_phraser)\nprint(ts, len(pkg_gz))\n\n# Stick the model creation timestamp into the name of the topic-terms file name\ntopic_object_name_ts = topic_object_name.replace('.csv','') + '.' + ts + '.csv'\nprint(topic_object_name_ts)", 
            "execution_count": 23, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "2018-01-03_03.51.10 2093283\nLDA_news.topic_terms.2018-01-03_03.51.10.csv\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "source": "# Write both files to COS\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + model_object_name, pkg_gz)\n\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + topic_object_name_ts, \n                '\\n'.join([str(t[0]) + \",\" + t[1] for t in topicTerms]))", 
            "execution_count": 24, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stderr", 
                    "text": "2018-01-03 03:51:50,157 : wstp.put_to_cos : WARNING : pyml/LDA_news.model.pkg.gz (2093283)\n2018-01-03 03:51:50,158 : wstp.put_to_cloud_object_storage : WARNING : my_data 2093283\n2018-01-03 03:51:52,099 : wstp.put_to_cos : WARNING : Response = <Response [200]>\n2018-01-03 03:51:52,100 : wstp.put_to_cos : WARNING : pyml/LDA_news.topic_terms.2018-01-03_03.51.10.csv (6361)\n2018-01-03 03:51:52,101 : wstp.put_to_cloud_object_storage : WARNING : my_data 6361\n2018-01-03 03:51:52,680 : wstp.put_to_cos : WARNING : Response = <Response [200]>\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# Use the trained LDA model to identify the top topics for newsgroup texts", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Before continuing...\n#### Open Streams Designer\ndo this \nand that", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Stream the dataset texts to Message Hub", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Create the Message Hub producer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "producer = wstp.create_messagehub_producer( username = mh_credentials['user'], password = mh_credentials['password'], kafka_brokers_sasl = mh_credentials['kafka_brokers_sasl'])", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### Send all of the text data to the MH topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "metadata": {}, 
            "source": "import time\n\ndata = read_dataset(dataset + \".gz\")\nfor i, entry in enumerate(data):\n    producer.send( mh_topic, { 'text': entry } )\n    if ((i+1) % 1000) == 0:\n        print(i+1, end=\" \")\n        time.sleep(1) # Slow things down during demo", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "python3-spark21", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "version": "3.5.2", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }
    }
}