{
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Create a Machine Learning Model for Unsupervised Text Classification\n## Introduction\nIn this notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Setup - Credentials and MH topic\nIn order to insert your credentials into the notebook, open the `Connections` tab to the right.\n* Use the notebook menu bar to open the `Data` panel on the right.\n* Select the `Connections` tab.\n\n#### Cloud Object Storage (COS) credentials\n\n* Click in the empty cell below and select <font color=blue>__insert to code__</font> for your __Cloud Object Storage__ service.\n* <font color=red>Change the *name*</font> of the COS credentials variable to __cos_credentials__.\n* The dictionary variable should contain the following keys: *iam_url, api_key, resource_instance_id, and url*.\n* Add an <font color=red>*additional key*</font> to the variable called __endpoint__.\n   * Get the the endpoint value from your Bluemix [Dashboard](https://console.bluemix.net/dashboard/apps) Cload Object Stora service's page.\n      * Select `Endpoint` from the menu on the left.\n      * Choose the __PUBLIC__ endpoint for your *location* (for example, *us-geo*).\n      * Prefix the endpoint value with \"https://\"\n\nYour variable should look like this...\n<div class=\"alert alert-block alert-success\">\n```\ncos_credentials = {\n  'iam_url':'<YOUR-VALUE>',\n  'api_key':'<YOUR-VALUE>',\n  'resource_instance_id':'<YOUR-VALUE>',\n  'url':'<YOUR-VALUE>',\n\n  'endpoint':'https://<YOUR-VALUE>'\n}```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "Choose a COS bucket name and object names for your packaged model (.gz) and topic-terms (.csv) files. \n<font color=red>Be sure that the bucket already exists!</font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "model_bucket_name = 'pyml'\nmodel_object_name = 'LDA_news.model.pkg.gz'\ntopic_object_name = 'LDA_news.topic_terms.csv'", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "#### Message Hub (MH) credentials\n\n* Click in the empty cell below and select <font color=blue>__insert to code__</font> for your __Message Hub__ service.\n* <font color=red>Change the *name*</font> of the MH credentials variable to __mh_credentials__.\n* The dictionary variable should contain at least the following keys: *username, password and brokers*.\n\nYour variable should look like this...\n<div class=\"alert alert-block alert-success\">\n```\nmh_credentials = {\n  'password':\"\"\"<YOUR-VALUE>\"\"\",\n  'brokers':'<YOUR-VALUE>',\n  'api_key':'<YOUR-VALUE>',\n  'username':'<YOUR-VALUE>'\n}```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 37
        }, 
        {
            "source": "Choose a Message Hub topic name.\n<font color=red>Be sure that the topic already exists!</font>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "mh_topic = 'pyml'", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 1
        }, 
        {
            "source": "# RAANON\ncos_credentials_prod_ki = {\n  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n  'api_key':'0s-JWmaDBwiSd_yWJqenoKRBfTVU5Rgkz31CDT5WgoWQ',\n  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/db0d062d2b4c0836e18618a5222d8068:22e3b946-6154-4032-8e8f-7cfb0b429602::',\n  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n      \"endpoint\":\"https://s3-api.us-geo.objectstorage.softlayer.net\",\n}\ncos_credentials_stage1_wd = {\n  'iam_url':'https://iam.stage1.ng.bluemix.net/oidc/token',\n  'api_key':'xhjheSC7AhSLtvapSDnbyFn17uWUqW5ccAOuHhQxnnEY',\n  'resource_instance_id':'crn:v1:staging:public:cloud-object-storage:global:a/68a66698d275aeb48097f868957ab2ed:bbb5aa36-5525-4000-b129-bcb780195098::',\n  'url':'https://s3-api.us-geo.objectstorage.uat.service.networklayer.com',\n    'endpoint':'https://s3.us-west.objectstorage.uat.softlayer.net'\n}\n\nmh_credentials_stage1_2s = {\n  \"instance_id\": \"81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-stage1.messagehub.services.us-south.bluemix.net/Lookup?serviceId=81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"api_key\": \"phXq2H0NSDQNSCdKGJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\",\n  \"kafka_admin_url\": \"https://kafka-admin-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-stage1.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"phXq2H0NSDQNSCdK\",\n  \"password\": \"GJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\",\n  \"username\": \"phXq2H0NSDQNSCdK\",\n  \"brokers\": 'kafka04-stage1.messagehub.services.us-south.bluemix.net:9093,kafka05-stage1.messagehub.services.us-south.bluemix.net:9093,kafka03-stage1.messagehub.services.us-south.bluemix.net:9093,kafka01-stage1.messagehub.services.us-south.bluemix.net:9093,kafka02-stage1.messagehub.services.us-south.bluemix.net:9093'\n}\n\n#cos_credentials = cos_credentials_prod_ki\ncos_credentials = cos_credentials_stage1_wd\nmh_credentials = mh_credentials_stage1_2s\n\nmh_topic = 'testTopic1'", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 3
        }, 
        {
            "source": "# Verify credential variables raise Exception('Missing value')\n#f = lambda v: [print(v) if len(v) == 0 for v in cos_credentials ]\nfor k in cos_credentials:\n    print('Missing value for', k) if len(cos_credentials.get(k)) == 0 else False", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 31
        }, 
        {
            "source": "# Setup - Download helper functions and the dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We've provided a package of helper function. Download and import it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!rm -f watson_streaming_pipelines.py*\n!wget https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\n# You may need this:\n#!pip install kafka\n\nimport watson_streaming_pipelines as wstp", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2018-01-03 04:47:41--  https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11971 (12K) [text/plain]\nSaving to: \u2018watson_streaming_pipelines.py\u2019\n\n100%[======================================>] 11,971      --.-K/s   in 0.001s  \n\n2018-01-03 04:47:41 (11.3 MB/s) - \u2018watson_streaming_pipelines.py\u2019 saved [11971/11971]\n\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 4
        }, 
        {
            "source": "### The dataset\nVersion 3.2 of gensim (December 2017) includes a mechanism for downloading some sample datasets (see https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/ and https://radimrehurek.com/gensim/downloader.html).\nEven if you have a previous version of gensim, you can still download the sample dataset we'll be using with the following cell (based on the source code at https://github.com/RaRe-Technologies/gensim/blob/master/gensim/downloader.py).\n#### Download the dataset from the gensim (RaRe-Technologies) github repository", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "DOWNLOAD_BASE_URL = \"https://github.com/RaRe-Technologies/gensim-data/releases/download\"\ndataset=\"20-newsgroups\"\n\n!rm -f {dataset}.gz*\n!wget '{DOWNLOAD_BASE_URL}/{dataset}/{dataset}.gz'\n!ls -l {dataset}.gz*", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "--2018-01-03 04:47:54--  https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/20-newsgroups.gz\nResolving github.com (github.com)... 192.30.253.113, 192.30.253.112\nConnecting to github.com (github.com)|192.30.253.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180103T104754Z&X-Amz-Expires=300&X-Amz-Signature=27ee2e329fb640bf652ecfdfc320e9e8ce01dcde339f664c84da4ca520f29718&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream [following]\n--2018-01-03 04:47:54--  https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180103T104754Z&X-Amz-Expires=300&X-Amz-Signature=27ee2e329fb640bf652ecfdfc320e9e8ce01dcde339f664c84da4ca520f29718&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream\nResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.230.163\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.230.163|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14483581 (14M) [application/octet-stream]\nSaving to: \u201820-newsgroups.gz\u2019\n\n100%[======================================>] 14,483,581  9.04MB/s   in 1.5s   \n\n2018-01-03 04:47:57 (9.04 MB/s) - \u201820-newsgroups.gz\u2019 saved [14483581/14483581]\n\n-rw------- 1 sca9-7277eb31bca08b-bc196c953de3 users 14483581 Nov  9 17:44 20-newsgroups.gz\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 5
        }, 
        {
            "source": "### function: read_dataset\nLoad the dataset and create a List of texts. (All stored in memory, so assume a small dataset.) \nThe dataset file should be in JSON format and contain a key called 'data'.\n\nParameters:\n* dataset_path: Path and filename of the dataset file.\n* max_lines: If greater than 0, abort reading the file after max_lines lines.\n\nReturns:\n* data: List of the text documents.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def read_dataset(dataset_path, max_lines=0):\n    \"\"\"\n    Read the dataset and return a List of each 'data' entry.\n    \"\"\"\n    from smart_open import smart_open\n    import json\n\n    print(\"opening...\", dataset_path)\n    \n    data = []\n    with smart_open( dataset_path, 'rb') as infile:\n        for i, line in enumerate(infile):\n            if max_lines > 0 and i == max_lines:\n                break\n            jsonData = json.loads(line.decode('utf8'))\n            data.append(jsonData['data'])\n        infile.close()\n\n    print(len(data), \"lines read\")\n\n    return data", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 6
        }, 
        {
            "source": "### function: preprocess_texts\nSteps to pre-process and cleanse texts:\n1. Stopword Removal.\n2. Collocation detection (bigram).\n3. Lemmatization (not stem since stemming can reduce the interpretability).\n    \nParameters:\n* texts: List of texts.\n* stoplist: List of stopword tokens (from nltk.corpus.stopwords.words('english')).\n* lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer()).    \n\nReturns:\n* tokens: Pre-processed tokenized texts.\n* bigram_phraser: The bigram phraser which was created using all of the training data.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Adapted from https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/gensim_news_classification.ipynb\ndef preprocess_texts(texts, stoplist, lemmatizer=None):\n\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    tokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                     if word not in stoplist]\n               for text in texts]\n\n    # bigram collocation detection\n    bigram = models.Phrases(tokens)\n    bigram_phraser = models.phrases.Phraser(bigram)\n    tokens = [bigram_phraser[text] for text in tokens]\n\n    if lemmatizer:\n        tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n\n    return tokens, bigram_phraser", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 7
        }, 
        {
            "source": "### function: train_model\nSteps to create the model\n1. Create a Dictionary using the List of cleansed tokenized text.\n2. [optional] Filter extremes.\n3. Create a corpus from the Bag-of-Words method.  \n    The BOW method takes the text tokens (words) and returns a list of tuples containing  \n    the word's token-id within the dictionary, and it's frequency within the input text.\n4. Create and train an LDA model. Play around with the hyperparameters to affect speed and quality.\n\nParameters:\n* textTokens: List of List of tokens, which are the cleansed text documents.\n\nResults:\n* model: The trained LDA model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def train_model( textTokens):\n\n    # Create the dictionary\n    dictionary = corpora.Dictionary( documents=textTokens)\n    \n    # Optional: Filter out tokens which are in less than 10 and more than 75.0% of the documents\n    dictionary.filter_extremes(no_below=10, no_above=0.75, keep_n=50000)\n\n    # The training corpus is the result of the Bag-of-Words method.\n    textBOW = [dictionary.doc2bow(text) for text in textTokens]\n\n    # Create the gensim LDA model - choose best arguments\n    model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dictionary,\n                                      num_topics=20, update_every=0.5,\n                                      # iterations=100, passes=3)\n                                      iterations=10, passes=1) # ONLY FOR FASTER TESTING\n\n    return model", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 8
        }, 
        {
            "source": "### function: package_mode\nPackage the model, phraser and creation timestamp into a pickled (serialized) and gzip-ed object.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def package_model( model, phraser):\n    import pickle, gzip\n    from time import strftime\n\n    timestamp = strftime('%Y-%m-%d_%H.%M.%S')\n    pkg = { 'timestamp': timestamp,\n            'model': model,\n            'phraser': phraser\n          }\n    pkg_gz = gzip.compress(pickle.dumps(pkg))\n    \n    return timestamp, pkg_gz", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 9
        }, 
        {
            "source": "# Begin work", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from gensim import models, corpora, utils\n###import importlib\n###importlib.reload(wstp)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n", 
                    "name": "stderr"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 10
        }, 
        {
            "source": "# Optional: Set optional_logging to True to set logging level. Set optional_logging to False to ignore this.\noptional_logging = False\nif optional_logging:\n    import logging, warnings\n    #Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\n    logging.basicConfig( level=logging.ERROR, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\n    logger = logging.getLogger()\n    logger.setLevel( logging.INFO)\n    wstp.setLogLevel( logging.INFO)\n    warnings.simplefilter('ignore')", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 11
        }, 
        {
            "source": "# Load the stoplist and lemmatizer from ntlk.download()\nstoplist = wstp.setStopWordList()\nlemmatizer = wstp.setLemmatizer()", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sca9-7277\n[nltk_data]     eb31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /gpfs/fs01/user/sca9-7277eb\n[nltk_data]     31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 12
        }, 
        {
            "source": "## TEST", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# TEST\ntexts = read_dataset(dataset + \".gz\", 3500)\n# Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\ntokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\n           for text in texts]\n\n# bigram collocation detection\nbigram = models.Phrases(tokens)\nbigram_phraser = models.phrases.Phraser(bigram)\ntokens = [bigram_phraser[text] for text in tokens]\n\n#if lemmatizer:\n#    tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "# TEST\nflat = [b for a in tokens for b in a if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\nprint(len(flat))\nu = sorted(set(flat))\nprint(len(u))\nprint(u[:10])", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "# TEST\ntopiclist = model.show_topics(num_topics=-1, num_words=20, formatted=False)\nflat = [b[0] for a in topiclist for b in a[1] if \"_\" in b[0] and not b[0].startswith(\"_\") and not b[0].endswith(\"_\")]\n#u = sorted(set(flat))\nprint(*sorted(set(flat)), sep='\\n')", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "# TEST\nimport logging\n#Log levels: CRITICAL=50, ERROR=40, WARNING=30, INFO=20, DEBUG=10, NOTSET=0\nlogging.basicConfig( level=logging.INFO, format='%(asctime)s : %(name)s.%(funcName)s : %(levelname)s : %(message)s')\nlogger = logging.getLogger()\nlogger.setLevel( logging.INFO)\n\ndef test1(tok):\n    print(tok)\n    flat = [b for b in tok if \"_\" in b and not b.startswith(\"_\") and not b.endswith(\"_\")]\n    print(len(flat))\n    u = sorted(set(flat))\n    print(len(u))\n    print(u)\n\n# Find baseball\nfor i,d in enumerate(texts):\n    if \"baseball\" in d and \"players\" in d:\n        print(i) #, texts[i])\n        break\ntesttokens = [word for word in utils.tokenize(texts[2383], lowercase=True, deacc=True, errors=\"ignore\")\n                 if word not in stoplist]\nprint(testtokens)\nprint(\"=================\")\ntest1(bigram_phraser[testtokens])\n\ntestbigram = models.Phrases([testtokens], min_count=1, threshold=2)\ntestbigram_phraser = models.phrases.Phraser(testbigram)\nprint(\"=================\")\ntest1(testbigram_phraser[testtokens])\n\n### SO, the bigger bigram_phraser found more bigrams !!!\n", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "### Train the model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# The 20-newsgroups dataset has 18846 entries. Let's take 3500 for training.\ntexts = read_dataset(dataset + \".gz\", 3500)\n\n# Pre-process and cleanse the texts\n%time textTokens, bigram_phraser = preprocess_texts( texts, stoplist, lemmatizer)\n\n# train the model\n%time model = train_model( textTokens)\n\n# Retrieve the topic terms from the model\ntopicTerms = model.print_topics(num_topics=-1, num_words=20)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "opening... 20-newsgroups.gz\n3500 lines read\nCPU times: user 11 s, sys: 310 ms, total: 11.3 s\nWall time: 11.4 s\nCPU times: user 19.6 s, sys: 21.4 s, total: 41 s\nWall time: 21.6 s\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 13
        }, 
        {
            "source": "# Display a sample of the topic terms List\nfor tt in topicTerms[:5]:\n    print(\"Topic={0}, Terms={1}\".format(tt[0],tt[1]))", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Topic=0, Terms=0.013*\"edu\" + 0.007*\"would\" + 0.006*\"one\" + 0.006*\"organization\" + 0.005*\"get\" + 0.005*\"think\" + 0.004*\"lines\" + 0.004*\"car\" + 0.004*\"writes\" + 0.004*\"also\" + 0.003*\"com\" + 0.003*\"well\" + 0.003*\"time\" + 0.003*\"back\" + 0.003*\"go\" + 0.003*\"like\" + 0.003*\"may\" + 0.003*\"new\" + 0.003*\"good\" + 0.003*\"engine\"\nTopic=1, Terms=0.006*\"players\" + 0.006*\"one\" + 0.005*\"edu\" + 0.005*\"lines\" + 0.005*\"would\" + 0.005*\"car\" + 0.005*\"good\" + 0.005*\"may\" + 0.004*\"think\" + 0.004*\"writes\" + 0.003*\"know\" + 0.003*\"also\" + 0.003*\"trade\" + 0.003*\"organization\" + 0.003*\"could\" + 0.003*\"time\" + 0.003*\"get\" + 0.003*\"god\" + 0.003*\"cars\" + 0.003*\"like\"\nTopic=2, Terms=0.008*\"edu\" + 0.007*\"winning\" + 0.006*\"think\" + 0.006*\"writes\" + 0.006*\"would\" + 0.006*\"one\" + 0.005*\"lines\" + 0.005*\"organization\" + 0.005*\"good\" + 0.004*\"team\" + 0.004*\"way\" + 0.004*\"god\" + 0.004*\"time\" + 0.004*\"well\" + 0.003*\"car\" + 0.003*\"c\" + 0.003*\"like\" + 0.003*\"get\" + 0.003*\"could\" + 0.003*\"people\"\nTopic=3, Terms=0.009*\"baseball\" + 0.007*\"edu\" + 0.007*\"one\" + 0.007*\"organization\" + 0.006*\"lines\" + 0.005*\"com\" + 0.005*\"would\" + 0.004*\"think\" + 0.004*\"writes\" + 0.004*\"good\" + 0.004*\"sale\" + 0.004*\"nntp_posting\" + 0.003*\"like\" + 0.003*\"car\" + 0.003*\"also\" + 0.003*\"two\" + 0.003*\"people\" + 0.003*\"even\" + 0.003*\"go\" + 0.003*\"know\"\nTopic=4, Terms=0.010*\"edu\" + 0.008*\"organization\" + 0.006*\"com\" + 0.006*\"lines\" + 0.005*\"one\" + 0.005*\"posting_host\" + 0.004*\"writes\" + 0.004*\"nntp_posting\" + 0.003*\"new\" + 0.003*\"c\" + 0.003*\"article\" + 0.003*\"reply\" + 0.003*\"host\" + 0.003*\"games\" + 0.003*\"also\" + 0.003*\"know\" + 0.003*\"car\" + 0.003*\"lines_nntp\" + 0.003*\"would\" + 0.003*\"like\"\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 14
        }, 
        {
            "source": "### Save the model and topic terms to Cloud Object Storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "ts, pkg_gz = package_model( model, bigram_phraser)\nprint(ts, len(pkg_gz))\n\n# Stick the model creation timestamp into the name of the topic-terms file name\ntopic_object_name_ts = topic_object_name.replace('.csv','') + '.' + ts + '.csv'\nprint(topic_object_name_ts)", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "2018-01-03_05.46.28 2093235\nLDA_news.topic_terms.2018-01-03_05.46.28.csv\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 15
        }, 
        {
            "source": "# Write both files to COS\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + model_object_name, pkg_gz)\n\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + topic_object_name_ts, \n                '\\n'.join([str(t[0]) + \",\" + t[1] for t in topicTerms]))", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 16
        }, 
        {
            "source": "# Use the trained LDA model to identify the top topics for newsgroup texts", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Create a Streams Flow using DSX Streams Designer\n* Download this sample Streams Flow __[LDA_Topic_Classification.stp](https://raw.githubusercontent.com/raanonr/DSX/master/pyML/LDA_Topic_Classification.stp)__.\n* Go to your *Data Science Experience* project.  \n* Choose the `Assets` tab and select &oplus;__New streams flow__, located on the right of *Streams flows*.  \n* Before filling any field, select the `From file` tab.  \n* In the bottom portion of the page, browse to (or drop) the Streams Flow which you downloaded.  \n* Make sure that your __Streams Analytics service__ is selected and then select __Create__.   ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This will open the flow pictured here \n![LDA_Topic_Classification](https://github.com/raanonr/DSX/blob/master/pyML/LDA_Topic_Classification_1.jpg?raw=true \"LDA_Topic_Classification\")", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This will open the flow pictured here  \n!  [LDA_Topic_Classification_1.jpg](attachment:LDA_Topic_Classification_1.jpg)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "You will notice the red mark on the Notifications (bell) icon on the right, since you have to adapt the flow for your credentials (etc).  \n* Select the *Edit the streams flow* (pencil) icon on the right.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Stream the dataset texts to Message Hub", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Create the Message Hub producer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "producer = wstp.create_messagehub_producer( username = mh_credentials['username'], password = mh_credentials['password'], kafka_brokers_sasl = mh_credentials['brokers'].split(','))", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "### Send all of the text data to the MH topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import time\n\ndata = read_dataset(dataset + \".gz\")\nfor i, entry in enumerate(data):\n    producer.send( mh_topic, { 'text': entry } )\n    if ((i+1) % 1000) == 0:\n        print(i+1, end=\" \")\n        time.sleep(1) # Slow things down during demo", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }
    }
}