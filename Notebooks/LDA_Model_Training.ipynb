{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Introduction\nIn this notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Setup - Credentials and MH topic\nThe Cloud Object Storage (COS) credentials should NOT come from the service's \"Service credentials\" page.  \nGet them from the \"insert to code\" option in the DSX Notebook \"Data -> Connections\" panel, on the right.\nThen ADD an entry to cos_credentials for the service's PUBLIC endpoint.  \nSpecify the bucket and object names for the model which will be created and the list of topics identified.  \nThe bucket MUST already exist in the COS service.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# @hidden_cell\n\ncos_credentials = {\n  'iam_url':'<REPLACE>',\n  'api_key':'<REPLACE>',\n  'resource_instance_id':'<REPLACE>',\n  'url':'<REPLACE>',\n\n  'endpoint':'<REPLACE>'\n}\n\n# <REPLACE> these sample values if needed\nmodel_bucket_name = 'pyml'\nmodel_object_name = 'LDA_news.model.pickle'\ntopic_object_name = 'LDA_news.topic_terms.csv'", 
            "metadata": {}, 
            "execution_count": 25, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "The Message Hub credentials should come from the service's \"Service credentials\" page.  \nSpecify your Message Hub topic name.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# @hidden_cell\n\nmh_credentials = {\n  \"kafka_brokers_sasl\": [\n      \"<REPLACE>\"\n  ],\n  \"user\": \"<REPLACE>\",\n  \"password\": \"<REPLACE>\"\n}\n\nmh_topic = '<REPLACE>'", 
            "metadata": {}, 
            "execution_count": 26, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# RAANON\ncos_credentials_prod_ki = {\n  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n  'api_key':'0s-JWmaDBwiSd_yWJqenoKRBfTVU5Rgkz31CDT5WgoWQ',\n  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/db0d062d2b4c0836e18618a5222d8068:22e3b946-6154-4032-8e8f-7cfb0b429602::',\n  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n      \"endpoint\":\"https://s3-api.us-geo.objectstorage.softlayer.net\",\n}\nmh_credentials_stage1_2s = {\n  \"instance_id\": \"81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"mqlight_lookup_url\": \"https://mqlight-lookup-stage1.messagehub.services.us-south.bluemix.net/Lookup?serviceId=81b7462e-7707-44c1-8bfa-8c9490ac8111\",\n  \"api_key\": \"phXq2H0NSDQNSCdKGJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\",\n  \"kafka_admin_url\": \"https://kafka-admin-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_rest_url\": \"https://kafka-rest-stage1.messagehub.services.us-south.bluemix.net:443\",\n  \"kafka_brokers_sasl\": [\n    \"kafka04-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka05-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka03-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka01-stage1.messagehub.services.us-south.bluemix.net:9093\",\n    \"kafka02-stage1.messagehub.services.us-south.bluemix.net:9093\"\n  ],\n  \"user\": \"phXq2H0NSDQNSCdK\",\n  \"password\": \"GJrEFTSnVHjgH8ugpChw1LgNbL3Sr23g\"\n}\n\ncos_credentials = cos_credentials_prod_ki\nmh_credentials = mh_credentials_stage1_2s\nmh_topic = 'testTopic1'", 
            "metadata": {}, 
            "execution_count": 27, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Setup - Download helper functions and the dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "We've provided a package of helper function. Download and import it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "!rm -f watson_streaming_pipelines.py*\n!wget https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\n# You may need this:\n#!pip install kafka\n\nimport watson_streaming_pipelines as wstp", 
            "metadata": {}, 
            "execution_count": 39, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-01 14:27:44--  https://raw.githubusercontent.com/raanonr/DSX/master/Notebooks/watson_streaming_pipelines.py\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 11971 (12K) [text/plain]\r\nSaving to: \u2018watson_streaming_pipelines.py\u2019\r\n\r\n\r 0% [                                       ] 0           --.-K/s              \r100%[======================================>] 11,971      --.-K/s   in 0.001s  \r\n\r\n2018-01-01 14:27:44 (9.77 MB/s) - \u2018watson_streaming_pipelines.py\u2019 saved [11971/11971]\r\n\r\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "#### The dataset\nVersion 3.2 of gensim (December 2017) includes a mechanism for downloading some sample datasets (see https://rare-technologies.com/new-api-for-pretrained-nlp-models-and-datasets-in-gensim/ and https://radimrehurek.com/gensim/downloader.html).\nEven if you have a previous version of gensim, you can still download the sample dataset we'll be using with the following cell (based on the source code at https://github.com/RaRe-Technologies/gensim/blob/master/gensim/downloader.py).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "DOWNLOAD_BASE_URL = \"https://github.com/RaRe-Technologies/gensim-data/releases/download\"\ndataset=\"20-newsgroups\"\n\n!rm -f {dataset}.gz*\n!wget '{DOWNLOAD_BASE_URL}/{dataset}/{dataset}.gz'\n!ls -l {dataset}.gz*", 
            "metadata": {}, 
            "execution_count": 32, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "--2018-01-01 14:17:13--  https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/20-newsgroups.gz\nResolving github.com (github.com)... 192.30.253.112, 192.30.253.113\nConnecting to github.com (github.com)|192.30.253.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180101T201714Z&X-Amz-Expires=300&X-Amz-Signature=14b02d0eace865f014abaf2139a3ee222642fe6abf53addb8e8e40018c5f4dcd&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream [following]\n--2018-01-01 14:17:14--  https://github-production-release-asset-2e65be.s3.amazonaws.com/106859079/d3f7d7ae-c5d1-11e7-960d-e92e1dc9279a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20180101%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180101T201714Z&X-Amz-Expires=300&X-Amz-Signature=14b02d0eace865f014abaf2139a3ee222642fe6abf53addb8e8e40018c5f4dcd&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3D20-newsgroups.gz&response-content-type=application%2Foctet-stream\nResolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.96.235\nConnecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.96.235|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14483581 (14M) [application/octet-stream]\nSaving to: \u201820-newsgroups.gz\u2019\n\n100%[======================================>] 14,483,581  4.29MB/s   in 3.2s   \n\n2018-01-01 14:17:17 (4.29 MB/s) - \u201820-newsgroups.gz\u2019 saved [14483581/14483581]\n\n-rw------- 1 sca9-7277eb31bca08b-bc196c953de3 users 14483581 Nov  9 17:44 20-newsgroups.gz\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "### function: read_dataset\nLoad the dataset and create a List of texts.\n(All stored in memory, so assuming a small dataset.)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def read_dataset(dataset_path, max_lines=0):\n    \"\"\"\n    Read the dataset and return a List of each 'data' entry.\n    \"\"\"\n    from smart_open import smart_open\n    import json\n\n    print(\"opening...\", dataset_path)\n    \n    data = []\n    with smart_open( dataset_path, 'rb') as infile:\n        for i, line in enumerate(infile):\n            if max_lines > 0 and i == max_lines:\n                break\n            jsonData = json.loads(line.decode('utf8'))\n            data.append(jsonData['data'])\n        infile.close()\n\n    print(len(data), \"lines read\")\n\n    return data", 
            "metadata": {}, 
            "execution_count": 48, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: preprocess_texts\nFunction to pre-process and cleanse texts. Following are the steps we take:  \n1. Stopword Removal.\n2. Collocation detection (bigram).\n3. Lemmatization (not stem since stemming can reduce the interpretability).\n    \nParameters:\n* texts: List of texts.\n* stoplist: list of stopword tokens (from nltk.corpus.stopwords.words('english'))\n* lemmatizer: [optional] Lemmatizer (from nltk.stem.WordNetLemmatizer())\n    \nReturns:\n* tokens: Pre-processed tokenized texts.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Adapted from https://github.com/RaRe-Technologies/gensim/blob/master/docs/notebooks/gensim_news_classification.ipynb\ndef preprocess_texts(texts, stoplist, lemmatizer=None):\n\n    # Convert to lowercase, remove accents, punctuation and digits. Tokenize and remove stop-words.\n    tokens = [[word for word in utils.tokenize(text, lowercase=True, deacc=True, errors=\"ignore\")\n                     if word not in stoplist]\n               for text in texts]\n\n    # bigram collocation detection\n    bigram = models.Phrases(tokens)\n    bigram_phraser = models.phrases.Phraser(bigram)\n    tokens = [bigram_phraser[text] for text in tokens]\n\n    if lemmatizer:\n        tokens = [[word for word in lemmatizer.lemmatize(' '.join(text), pos='v').split()] for text in tokens]\n\n    return tokens", 
            "metadata": {}, 
            "execution_count": 44, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### function: trail_model\nSteps to create the model\n1. Create a Dictionary using the List of cleansed tokenized text.\n2. [optional] Filter extremes.\n3. Create a corpus from the Bag-of-Words method.\n4. Create and train an LDA model. Play around with the hyperparameters to affect speed and quality.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "def train_model( textTokens):\n\n    # Create the dictionary\n    dictionary = corpora.Dictionary( documents=textTokens)\n    # Optional: Filter out tokens which are in less than 5 and more than 90.0% of the documents\n    dictionary.filter_extremes(no_below=10, no_above=0.75, keep_n=100000)\n\n    # The training corpus is the result of the Bag-of-Words method.\n    # The BOW method takes the text tokens (words) and returns a list of tuples containing\n    # the word's token-id within the dictionary, and it's frequency within the input text.\n    textBOW = [dictionary.doc2bow(text) for text in textTokens]\n\n    # Create the gensim LDA model - choose best arguments\n    model = models.ldamodel.LdaModel( corpus=textBOW, id2word=dictionary,\n                                      num_topics=20, update_every=0.5,\n#                                      iterations=100, passes=3)\n                                      iterations=10, passes=1) # ONLY FOR FASTER TESTING\n\n    return model", 
            "metadata": {}, 
            "execution_count": 36, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "# Begin work", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from gensim import models, corpora, utils\n###import importlib\n###importlib.reload(wstp)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "stoplist = wstp.setStopWordList()\nlemmatizer = wstp.setLemmatizer()", 
            "metadata": {}, 
            "execution_count": 42, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "[nltk_data] Downloading package stopwords to /gpfs/fs01/user/sca9-7277\n[nltk_data]     eb31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /gpfs/fs01/user/sca9-7277eb\n[nltk_data]     31bca08b-bc196c953de3/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "# The 20-newsgroups dataset has 18846 entries. Let's take 3500 for training.\ntexts = read_dataset(dataset + \".gz\", 3500)\n\ntextTokens = preprocess_texts( texts, stoplist, lemmatizer)\n%time model = train_model( textTokens)\n\n# Retrieve the topic terms from the model to include with the returned output\ntopicTerms = model.print_topics(num_topics=-1, num_words=20)", 
            "metadata": {}, 
            "execution_count": 56, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "opening... 20-newsgroups.gz\n3500 lines read\nCPU times: user 20.1 s, sys: 36.8 s, total: 56.9 s\nWall time: 24.3 s\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "wstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + model_object_name, \n                wstp.pickleSerializer( data=model, zip=True))\nwstp.put_to_cos( cos_credentials, model_bucket_name + \"/\" + topic_object_name, \n                '\\n'.join([str(t[0]) + \",\" + t[1] for t in topicTerms]))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### Create the Message Hub producer", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "producer = stp.create_messagehub_producer( username = mh_credentials['user'], password = mh_credentials['password'], kafka_brokers_sasl = mh_credentials['kafka_brokers_sasl'])", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "### Send all of the text data to the MH topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import time\n\n#data = read_dataset(dataset + \".gz\")\nfor i, entry in enumerate(data[:2000]):\n    producer.send( mh_topic, { 'text': entry['text'] } )\n    if ((i+1) % 1000) == 0:\n        print(i+1, end=\" \")\n        #time.sleep(1) # Slow things down during demo", 
            "metadata": {}, 
            "execution_count": 133, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "1000 2000 ", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "name": "python3-spark21", 
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1"
        }, 
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "version": "3.5.2"
        }
    }
}